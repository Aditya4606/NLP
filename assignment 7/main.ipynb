{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce0d8ed",
   "metadata": {},
   "source": [
    "<h1>1. Using the bigram and unigram language models trained on the training data from the\n",
    "Assignment 1, compute the PMI scores for all the bigrams in the validation and testing\n",
    "sets created from Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c61fe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PMI Calculator...\n",
      "Initialization complete.\n",
      "\n",
      "Processing validation.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PMI for validation.txt: 1000it [00:00, 21809.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI scores saved to validation_pmi_scores.csv\n",
      "\n",
      "Processing test.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating PMI for test.txt: 1000it [00:00, 24627.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI scores saved to test_pmi_scores.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import math\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_ngram_counts(n):\n",
    "    file_name = f\"ngram_counts_{n}gram.pkl\"\n",
    "    try:\n",
    "        with open(file_name, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_name} was not found.\")\n",
    "        exit()\n",
    "\n",
    "class PMICalculator:\n",
    "    def __init__(self, unigram_counts, bigram_counts):\n",
    "        print(\"Initializing PMI Calculator...\")\n",
    "        self.unigrams = unigram_counts\n",
    "        self.bigrams = bigram_counts\n",
    "\n",
    "        self.total_unigram_tokens = sum(self.unigrams.values())\n",
    "        self.total_bigram_tokens = sum(self.bigrams.values())\n",
    "\n",
    "        if self.total_unigram_tokens == 0 or self.total_bigram_tokens == 0:\n",
    "            raise ValueError(\"N-gram count files must not be empty.\")\n",
    "        \n",
    "        print(\"Initialization complete.\")\n",
    "\n",
    "    def get_pmi(self, w1, w2):\n",
    "        \"\"\"\n",
    "        PMI(w1, w2) = log2( P(w1, w2) / (P(w1) * P(w2)) )\n",
    "        \"\"\"\n",
    "        unigram1 = (w1,)\n",
    "        unigram2 = (w2,)\n",
    "        bigram = (w1, w2)\n",
    "\n",
    "        # Get counts from the training data\n",
    "        count_w1 = self.unigrams.get(unigram1, 0)\n",
    "        count_w2 = self.unigrams.get(unigram2, 0)\n",
    "        count_w1_w2 = self.bigrams.get(bigram, 0)\n",
    "\n",
    "        # If any word or the bigram itself was unseen in training, PMI is undefined.\n",
    "        # We represent this with negative infinity.\n",
    "        if count_w1 == 0 or count_w2 == 0 or count_w1_w2 == 0:\n",
    "            return -float('inf')\n",
    "\n",
    "        p_w1 = count_w1 / self.total_unigram_tokens\n",
    "        p_w2 = count_w2 / self.total_unigram_tokens\n",
    "        p_w1_w2 = count_w1_w2 / self.total_bigram_tokens\n",
    "\n",
    "        # PMI\n",
    "        pmi = math.log2(p_w1_w2 / (p_w1 * p_w2))\n",
    "        return pmi\n",
    "\n",
    "\n",
    "def process_file(pmi_calculator, input_filename, output_filename):\n",
    "    print(f\"\\nProcessing {input_filename}...\")\n",
    "    with open(input_filename, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_filename, 'w', encoding='utf-8', newline='') as outfile:\n",
    "        \n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(['word1', 'word2', 'pmi_score'])\n",
    "\n",
    "        for line in tqdm(infile, desc=f\"Calculating PMI for {input_filename}\"):\n",
    "            words = line.strip().split()\n",
    "            if len(words) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Generate all bigrams from the sentence\n",
    "            for i in range(len(words) - 1):\n",
    "                w1, w2 = words[i], words[i+1]\n",
    "                pmi_score = pmi_calculator.get_pmi(w1, w2)\n",
    "                writer.writerow([w1, w2, pmi_score])\n",
    "                \n",
    "    print(f\"PMI scores saved to {output_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unigrams = load_ngram_counts(1)\n",
    "    bigrams = load_ngram_counts(2)\n",
    "\n",
    "    pmi_calc = PMICalculator(unigrams, bigrams)\n",
    "\n",
    "    process_file(pmi_calc, \"validation.txt\", \"validation_pmi_scores.csv\")\n",
    "    process_file(pmi_calc, \"test.txt\", \"test_pmi_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad113c",
   "metadata": {},
   "source": [
    "<h1>Vectorize all the sentences in the training, validation, and testing data that you tokenized\n",
    "from Assignment-1 using TF-IDF. For the validation and testing data, use the IDF scores\n",
    "learned from the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d917fd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [['તેની', 'વિગત', 'જાણવી', 'આજે', 'પુસ્તુત', 'પણ', 'છે', 'અને', 'અનુકરણીય', 'પણ', 'આ', 'સ્થળે', '૩૫૦૦', 'એકર', 'ઉજજડ', 'જમીનમાં', '૪૦', 'હજાર', 'જેટલાં', 'નિરાશ્રિતો', 'તંબુ', 'બાંધીને', 'રહેતાં', 'હતાં.'], ['જેમા', 'એક', 'ગુટ', 'તો', 'રૃબરૃ', 'રજૂઆત', 'કરવા', 'માટે', 'છેક', 'પાવર', 'પોઈન્ટ', 'ગાંધીનગર', 'સુધી', 'લાંબુ', 'થયુ', 'હતું.']]\n",
      "Validation: [['વાસ્તવિક', 'અંકુશ', 'રેખા', 'નજીક', 'આવેલા', 'પેન્ગોગ', 'ત્સો', 'એટલે', 'કે', 'સરોવર', 'સુધી', 'પહોંચવું', 'અત્યારે', 'પણ', 'ઘણું', 'મુશ્કેલ', 'છે.'], ['#1.']]\n",
      "Test: [['આગળ', 'રાજીવે', 'કહ્યું', 'કે', 'મને', 'આ', 'ફિલ્મ', 'કરવાનો', 'આનંદ', 'એટલે', 'થયો', 'કારણ', 'કે', 'હું', 'અમિતાભની', 'પુજા', 'કરૂ', 'છું', 'અને', 'એનાં', 'દાયકાનાં', 'બધા', 'કલાકારોનો', 'આદર', 'કરૂ', 'છું.'], ['અને', 'કરેલાં', 'વ્યવહારો', 'કઈ', 'રીતે', 'દર્શાવવા,', 'જેથી', 'કર', 'અને', 'પેનલ્ટીમાં', 'રાહત', 'મળી', 'શકે', '?']]\n"
     ]
    }
   ],
   "source": [
    "# Function to read sentences from a .txt file and tokenize\n",
    "def read_and_tokenize(filename):\n",
    "    sentences = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Strip leading/trailing whitespace and split by space\n",
    "            tokens = line.strip().split()\n",
    "            if tokens:  # skip empty lines\n",
    "                sentences.append(tokens)\n",
    "    return sentences\n",
    "\n",
    "# Example usage\n",
    "train_sentences = read_and_tokenize(\"/Users/adityakumar/Desktop/college_labs/NLP/assignment 7/train.txt\")\n",
    "validation_sentences = read_and_tokenize(\"/Users/adityakumar/Desktop/college_labs/NLP/assignment 7/validation.txt\")\n",
    "test_sentences = read_and_tokenize(\"/Users/adityakumar/Desktop/college_labs/NLP/assignment 7/test.txt\")\n",
    "\n",
    "# Check first few sentences\n",
    "print(\"Train:\", train_sentences[:2])\n",
    "print(\"Validation:\", validation_sentences[:2])\n",
    "print(\"Test:\", test_sentences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cedb2afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Build DF (document frequency) from training data\n",
    "def compute_df(sentences):\n",
    "    df = defaultdict(int)\n",
    "    for sent in sentences:\n",
    "        unique_tokens = set(sent)\n",
    "        for token in unique_tokens:\n",
    "            df[token] += 1\n",
    "    return df\n",
    "\n",
    "train_df = compute_df(train_sentences)\n",
    "N_train = len(train_sentences)  # total number of training sentences\n",
    "\n",
    "# Compute IDF from DF\n",
    "idf = {}\n",
    "for word, freq in train_df.items():\n",
    "    idf[word] = math.log((N_train + 1) / (freq + 1)) + 1  # smoothed IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcd00c8",
   "metadata": {},
   "source": [
    "<h1>tf-idf for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e3130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf(sentences, idf_dict):\n",
    "    tfidf_vectors = []\n",
    "    for sent in sentences:\n",
    "        tf = defaultdict(int)\n",
    "        for token in sent:\n",
    "            tf[token] += 1\n",
    "        # Normalize TF by sentence length\n",
    "        tf_normalized = {word: count / len(sent) for word, count in tf.items()}\n",
    "        # Compute TF-IDF\n",
    "        tfidf = {word: tf_normalized[word] * idf_dict.get(word, math.log((N_train+1)/1)+1)\n",
    "                 for word in tf_normalized}  # unknown words get minimal IDF\n",
    "        tfidf_vectors.append(tfidf)\n",
    "    return tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13097de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'તેની': 0.2081278205151952, 'વિગત': 0.32838807423750926, 'જાણવી': 0.4600118240628153, 'આજે': 0.2128292066461221, 'પુસ્તુત': 0.7145329310436339, 'પણ': 0.2712827450057881, 'છે': 0.12962369091970113, 'અને': 0.1059952708832195, 'અનુકરણીય': 0.497093247790335, 'આ': 0.12048087663425744, 'સ્થળે': 0.2997364744699214, '૩૫૦૦': 0.4729883306653107, 'એકર': 0.3906506943568616, 'ઉજજડ': 0.60359177158424, 'જમીનમાં': 0.35845603796074227, '૪૦': 0.33521745364434646, 'હજાર': 0.26778155010653365, 'જેટલાં': 0.39435915068008837, 'નિરાશ્રિતો': 0.6016975059977085, 'તંબુ': 0.46555357572991507, 'બાંધીને': 0.39809338997822113, 'રહેતાં': 0.3475133977362111, 'હતાં.': 0.26507229153366646}\n"
     ]
    }
   ],
   "source": [
    "train_tfidf = compute_tf_idf(train_sentences, idf)\n",
    "validation_tfidf = compute_tf_idf(validation_sentences, idf)\n",
    "test_tfidf = compute_tf_idf(test_sentences, idf)\n",
    "\n",
    "# Example: print TF-IDF of first training sentence\n",
    "print(train_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b363602",
   "metadata": {},
   "source": [
    "<h1>convert tf-idf to aligned vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9382efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix, save_npz, load_npz\n",
    "\n",
    "def tfidf_dicts_to_sparse(tfidf_list, vocab):\n",
    "    \"\"\"\n",
    "    Converts a list of TF-IDF dictionaries to a sparse LIL matrix.\n",
    "    \"\"\"\n",
    "    matrix = lil_matrix((len(tfidf_list), len(vocab)), dtype=float)\n",
    "    word_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    for i, tfidf in enumerate(tfidf_list):\n",
    "        for word, value in tfidf.items():\n",
    "            if word in word_index:\n",
    "                matrix[i, word_index[word]] = value\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4812a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vocabulary built successfully with 6226458 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Generate vocabulary dictionary from training TF-IDF data\n",
    "# ------------------------------\n",
    "\n",
    "# Create an empty vocab dictionary\n",
    "vocab = {}\n",
    "\n",
    "# Go through each training TF-IDF dictionary and assign an index to each unique token\n",
    "for doc in train_tfidf:\n",
    "    for word in doc.keys():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "print(f\"✅ Vocabulary built successfully with {len(vocab)} unique tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "818010ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sparse = tfidf_dicts_to_sparse(train_tfidf, vocab).tocsr()\n",
    "validation_sparse = tfidf_dicts_to_sparse(validation_tfidf, vocab).tocsr()\n",
    "test_sparse = tfidf_dicts_to_sparse(test_tfidf, vocab).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "538d1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz(\"train_tfidf.npz\", train_sparse)\n",
    "save_npz(\"validation_tfidf.npz\", validation_sparse)\n",
    "save_npz(\"test_tfidf.npz\", test_sparse)\n",
    "\n",
    "# To load later:\n",
    "validation_sparse = load_npz(\"validation_tfidf.npz\")\n",
    "test_sparse = load_npz(\"test_tfidf.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41009215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Files loaded successfully.\n",
      "Train vectors shape:      (30935118, 6226458)\n",
      "Validation vectors shape: (1000, 6226458)\n",
      "Test vectors shape:       (1000, 6226458)\n",
      "\n",
      "--- First Training Vector (Sparse Format) ---\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 23 stored elements and shape (1, 6226458)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t0.2081278205151952\n",
      "  (0, 1)\t0.32838807423750926\n",
      "  (0, 2)\t0.4600118240628153\n",
      "  (0, 3)\t0.2128292066461221\n",
      "  (0, 4)\t0.7145329310436339\n",
      "  (0, 5)\t0.2712827450057881\n",
      "  (0, 6)\t0.12962369091970113\n",
      "  (0, 7)\t0.1059952708832195\n",
      "  (0, 8)\t0.497093247790335\n",
      "  (0, 9)\t0.12048087663425744\n",
      "  (0, 10)\t0.2997364744699214\n",
      "  (0, 11)\t0.4729883306653107\n",
      "  (0, 12)\t0.3906506943568616\n",
      "  (0, 13)\t0.60359177158424\n",
      "  (0, 14)\t0.35845603796074227\n",
      "  (0, 15)\t0.33521745364434646\n",
      "  (0, 16)\t0.26778155010653365\n",
      "  (0, 17)\t0.39435915068008837\n",
      "  (0, 18)\t0.6016975059977085\n",
      "  (0, 19)\t0.46555357572991507\n",
      "  (0, 20)\t0.39809338997822113\n",
      "  (0, 21)\t0.3475133977362111\n",
      "  (0, 22)\t0.26507229153366646\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import load_npz\n",
    "\n",
    "# Define the filenames\n",
    "train_file = \"train_tfidf.npz\"\n",
    "validation_file = \"validation_tfidf.npz\"\n",
    "test_file = \"test_tfidf.npz\"\n",
    "\n",
    "try:\n",
    "    # Load the sparse matrices from the .npz files\n",
    "    train_vectors = load_npz(train_file)\n",
    "    validation_vectors = load_npz(validation_file)\n",
    "    test_vectors = load_npz(test_file)\n",
    "\n",
    "    # Print the shapes to confirm they loaded correctly\n",
    "    print(\"✅ Files loaded successfully.\")\n",
    "    print(f\"Train vectors shape:      {train_vectors.shape}\")\n",
    "    print(f\"Validation vectors shape: {validation_vectors.shape}\")\n",
    "    print(f\"Test vectors shape:       {test_vectors.shape}\")\n",
    "\n",
    "    # Example: Print the first vector from the training data\n",
    "    # This will show its sparse representation (column indices and TF-IDF values)\n",
    "    print(\"\\n--- First Training Vector (Sparse Format) ---\")\n",
    "    print(train_vectors[0])\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error: Could not find the file.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    print(\"Please make sure the .npz files are in the same directory as this script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf833f",
   "metadata": {},
   "source": [
    "<h1>For each sentence in the validation and testing sets, find its nearest neighbor in that set\n",
    "using TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b966c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finding Nearest Neighbors using TF-IDF Vectors ---\n",
      "\n",
      "Loading TF-IDF vectors and original sentences...\n",
      "\n",
      "--- Processing Validation Set ---\n",
      "\n",
      "Showing 5 example sentences and their nearest neighbors from the validation set:\n",
      "\n",
      "Original Sentence (0):\n",
      "  'વાસ્તવિક અંકુશ રેખા નજીક આવેલા પેન્ગોગ ત્સો એટલે કે સરોવર સુધી પહોંચવું અત્યારે પણ ઘણું મુશ્કેલ છે.'\n",
      "Nearest Neighbor (sentence 750):\n",
      "  'અરુણાચલ પ્રદેશમાં ભારત અને ચીન વાસ્તવિક નિયંત્રણ રેખા પર સેનાના જવાનો સામસામે આવી ગયા હતા.'\n",
      "Cosine Similarity: 0.1514\n",
      "--------------------------------------------------\n",
      "Original Sentence (1):\n",
      "  '#1.'\n",
      "Nearest Neighbor (sentence 0):\n",
      "  'વાસ્તવિક અંકુશ રેખા નજીક આવેલા પેન્ગોગ ત્સો એટલે કે સરોવર સુધી પહોંચવું અત્યારે પણ ઘણું મુશ્કેલ છે.'\n",
      "Cosine Similarity: 0.0000\n",
      "--------------------------------------------------\n",
      "Original Sentence (2):\n",
      "  'અને બીજાં નવા નિશાળિયા.'\n",
      "Nearest Neighbor (sentence 239):\n",
      "  'આઈપીએલ-14 અગાઉ ટીમો નવા ખેલાડીઓ ખરીદવા માટે આતુર છે.'\n",
      "Cosine Similarity: 0.1090\n",
      "--------------------------------------------------\n",
      "Original Sentence (3):\n",
      "  'આ ગામના વાલીઓએ ફીની બબાલથી બચવા કરી અનોખી પહેલ.'\n",
      "Nearest Neighbor (sentence 75):\n",
      "  'આમ અનોખી રીતે આદિવાસી સમાજમાં ચૌદશના તહેવારની ઉજવણી કરાય છે.'\n",
      "Cosine Similarity: 0.1052\n",
      "--------------------------------------------------\n",
      "Original Sentence (4):\n",
      "  '74 ડિગ્રી ઉત્તર અક્ષાંશ અને 104.'\n",
      "Nearest Neighbor (sentence 529):\n",
      "  'જણાવી દઈએ કે માયાવતીએ ઉત્તર પ્રદેશમાં બસપા શાસનકાળણાં કેટલા પાર્કનું નિર્માણ કરાવ્યું હતું.'\n",
      "Cosine Similarity: 0.0626\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "--- Processing Test Set ---\n",
      "\n",
      "Showing 5 example sentences and their nearest neighbors from the test set:\n",
      "\n",
      "Original Sentence (0):\n",
      "  'આગળ રાજીવે કહ્યું કે મને આ ફિલ્મ કરવાનો આનંદ એટલે થયો કારણ કે હું અમિતાભની પુજા કરૂ છું અને એનાં દાયકાનાં બધા કલાકારોનો આદર કરૂ છું.'\n",
      "Nearest Neighbor (sentence 458):\n",
      "  'હાલ હું હોમ ક્વોરન્ટીન છું.'\n",
      "Cosine Similarity: 0.1060\n",
      "--------------------------------------------------\n",
      "Original Sentence (1):\n",
      "  'અને કરેલાં વ્યવહારો કઈ રીતે દર્શાવવા, જેથી કર અને પેનલ્ટીમાં રાહત મળી શકે ?'\n",
      "Nearest Neighbor (sentence 331):\n",
      "  'વાંક કોનો છે ?'\n",
      "Cosine Similarity: 0.0852\n",
      "--------------------------------------------------\n",
      "Original Sentence (2):\n",
      "  'વાસ્તવિક ફેશન વલણ ચળકતા હાથ છે, જે ચમકતા સોના અથવા મેટલ વર્ઝનમાં બનાવવામાં આવે છે.'\n",
      "Nearest Neighbor (sentence 601):\n",
      "  'ઓમેગા -3 પૂરવણીઓ (સામાન્ય રીતે માછલીમાંથી બનાવવામાં આવે છે, પરંતુ કેટલીક વખત શેલફીશમાંથી બનાવવામાં આવે છે)'\n",
      "Cosine Similarity: 0.1488\n",
      "--------------------------------------------------\n",
      "Original Sentence (3):\n",
      "  'આ ઉપરાંત એક્ટ્રસ બાની અને લીઝા રેની વચ્ચેના બોલ્ડ સીન્સ પણ ચર્ચામાં રહ્યા હતા.'\n",
      "Nearest Neighbor (sentence 275):\n",
      "  'બધા કલાકારો આવી રહ્યા હતા.'\n",
      "Cosine Similarity: 0.1012\n",
      "--------------------------------------------------\n",
      "Original Sentence (4):\n",
      "  'પાણપુરની મોબાઇલ શોપમાંથી રૂપિયા 70 હજારના મોબાઇલની ચોરી'\n",
      "Nearest Neighbor (sentence 48):\n",
      "  'ગડસીસરમાં મોબાઇલની બેટરી ફાટી, સદ્દનસીબે જાનહાનિ ટળી'\n",
      "Cosine Similarity: 0.1762\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def load_sentences(filepath):\n",
    "    \"\"\"Reads a file and returns a list of sentences.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        sentences = [line.strip() for line in f if line.strip()]\n",
    "    return sentences\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Finding Nearest Neighbors using TF-IDF Vectors ---\")\n",
    "\n",
    "    try:\n",
    "        print(\"\\nLoading TF-IDF vectors and original sentences...\")\n",
    "        validation_vectors = load_npz(\"tfidf_validation_vectors.npz\")\n",
    "        test_vectors = load_npz(\"tfidf_test_vectors.npz\")\n",
    "        \n",
    "        validation_sentences = load_sentences(\"validation.txt\")\n",
    "        test_sentences = load_sentences(\"test.txt\")\n",
    "\n",
    "        print(\"\\n--- Processing Validation Set ---\")\n",
    "        \n",
    "        # 'cosine' metric is used for TF-IDF. The algorithm will find vectors with the smallest angle.\n",
    "        nn_model_val = NearestNeighbors(n_neighbors=2, metric='cosine', algorithm='brute')\n",
    "        nn_model_val.fit(validation_vectors)\n",
    "\n",
    "        # Find the neighbors for all sentences in the validation set\n",
    "        distances_val, indices_val = nn_model_val.kneighbors(validation_vectors)\n",
    "        \n",
    "        print(\"\\nShowing 5 example sentences and their nearest neighbors from the validation set:\\n\")\n",
    "        for i in range(5):\n",
    "            original_sentence = validation_sentences[i]\n",
    "            neighbor_index = indices_val[i][1]\n",
    "            neighbor_sentence = validation_sentences[neighbor_index]\n",
    "            # The model returns cosine distance. Similarity = 1 - distance.\n",
    "            similarity_score = 1 - distances_val[i][1]\n",
    "            \n",
    "            print(f\"Original Sentence ({i}):\\n  '{original_sentence}'\")\n",
    "            print(f\"Nearest Neighbor (sentence {neighbor_index}):\\n  '{neighbor_sentence}'\")\n",
    "            print(f\"Cosine Similarity: {similarity_score:.4f}\\n\" + \"-\"*50)\n",
    "\n",
    "\n",
    "        # --- 3. Find Nearest Neighbors in the Test Set ---\n",
    "        print(\"\\n\\n--- Processing Test Set ---\")\n",
    "        nn_model_test = NearestNeighbors(n_neighbors=2, metric='cosine', algorithm='brute')\n",
    "        nn_model_test.fit(test_vectors)\n",
    "\n",
    "        distances_test, indices_test = nn_model_test.kneighbors(test_vectors)\n",
    "\n",
    "        print(\"\\nShowing 5 example sentences and their nearest neighbors from the test set:\\n\")\n",
    "        for i in range(5):\n",
    "            original_sentence = test_sentences[i]\n",
    "            neighbor_index = indices_test[i][1]\n",
    "            neighbor_sentence = test_sentences[neighbor_index]\n",
    "            similarity_score = 1 - distances_test[i][1]\n",
    "            \n",
    "            print(f\"Original Sentence ({i}):\\n  '{original_sentence}'\")\n",
    "            print(f\"Nearest Neighbor (sentence {neighbor_index}):\\n  '{neighbor_sentence}'\")\n",
    "            print(f\"Cosine Similarity: {similarity_score:.4f}\\n\" + \"-\"*50)\n",
    "\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nError: {e}.\")\n",
    "        print(\"Please make sure you have successfully run the 'tfidf_vectorizer.py' script first,\")\n",
    "        print(\"and that the .npz and .txt files are in the same directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e77cae27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finding Nearest Neighbors using TF-IDF Vectors ---\n",
      "\n",
      "Loading TF-IDF vectors and original sentences...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Training Set Subset ---\n",
      "Saving all 10000 training subset neighbors to train_subset_nearest_neighbors.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Subset Neighbors: 100%|██████████| 10000/10000 [00:00<00:00, 230351.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Validation Set ---\n",
      "Saving all validation neighbors to validation_nearest_neighbors.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Neighbors: 100%|██████████| 1000/1000 [00:00<00:00, 214740.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Processing Test Set ---\n",
      "Saving all test neighbors to test_nearest_neighbors.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Neighbors: 100%|██████████| 1000/1000 [00:00<00:00, 175839.68it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import csv\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------- Helper Functions -----------------\n",
    "\n",
    "def load_sentences(filepath, limit=None):\n",
    "    \"\"\"Reads a file and returns a list of sentences, with an optional limit.\"\"\"\n",
    "    sentences = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if limit and i >= limit:\n",
    "                break\n",
    "            if line.strip():\n",
    "                sentences.append(line.strip())\n",
    "    return sentences\n",
    "\n",
    "# ----------------- Main Execution -----------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Finding Nearest Neighbors using TF-IDF Vectors ---\")\n",
    "\n",
    "    try:\n",
    "        # --- 1. Load the TF-IDF vectors and the original text sentences ---\n",
    "        print(\"\\nLoading TF-IDF vectors and original sentences...\")\n",
    "        train_vectors_full = load_npz(\"tfidf_train_vectors.npz\")\n",
    "        validation_vectors = load_npz(\"tfidf_validation_vectors.npz\")\n",
    "        test_vectors = load_npz(\"tfidf_test_vectors.npz\")\n",
    "        \n",
    "        # We will process a subset of the training data to get 10,000 lines\n",
    "        NUM_TRAIN_SAMPLES = 10000\n",
    "        train_sentences_subset = load_sentences(\"train.txt\", limit=NUM_TRAIN_SAMPLES)\n",
    "        train_vectors_subset = train_vectors_full[:NUM_TRAIN_SAMPLES]\n",
    "\n",
    "        validation_sentences = load_sentences(\"validation.txt\")\n",
    "        test_sentences = load_sentences(\"test.txt\")\n",
    "\n",
    "        # --- 2. Find Nearest Neighbors in the Training Subset ---\n",
    "        print(\"\\n--- Processing Training Set Subset ---\")\n",
    "        nn_model_train = NearestNeighbors(n_neighbors=2, metric='cosine', algorithm='brute')\n",
    "        nn_model_train.fit(train_vectors_subset)\n",
    "        distances_train, indices_train = nn_model_train.kneighbors(train_vectors_subset)\n",
    "\n",
    "        output_train_filename = \"train_subset_nearest_neighbors.csv\"\n",
    "        with open(output_train_filename, 'w', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['original_sentence', 'nearest_neighbor', 'cosine_similarity'])\n",
    "            \n",
    "            print(f\"Saving all {NUM_TRAIN_SAMPLES} training subset neighbors to {output_train_filename}...\")\n",
    "            for i in tqdm(range(len(train_sentences_subset)), desc=\"Training Subset Neighbors\"):\n",
    "                original_sentence = train_sentences_subset[i]\n",
    "                neighbor_index = indices_train[i][1]\n",
    "                neighbor_sentence = train_sentences_subset[neighbor_index]\n",
    "                similarity_score = 1 - distances_train[i][1]\n",
    "                writer.writerow([original_sentence, neighbor_sentence, f\"{similarity_score:.4f}\"])\n",
    "        \n",
    "        # --- 3. Find Nearest Neighbors in the Validation Set ---\n",
    "        print(\"\\n--- Processing Validation Set ---\")\n",
    "        nn_model_val = NearestNeighbors(n_neighbors=2, metric='cosine', algorithm='brute')\n",
    "        nn_model_val.fit(validation_vectors)\n",
    "        distances_val, indices_val = nn_model_val.kneighbors(validation_vectors)\n",
    "        \n",
    "        output_val_filename = \"validation_nearest_neighbors.csv\"\n",
    "        with open(output_val_filename, 'w', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['original_sentence', 'nearest_neighbor', 'cosine_similarity'])\n",
    "            \n",
    "            print(f\"Saving all validation neighbors to {output_val_filename}...\")\n",
    "            for i in tqdm(range(len(validation_sentences)), desc=\"Validation Neighbors\"):\n",
    "                original_sentence = validation_sentences[i]\n",
    "                neighbor_index = indices_val[i][1]\n",
    "                neighbor_sentence = validation_sentences[neighbor_index]\n",
    "                similarity_score = 1 - distances_val[i][1]\n",
    "                writer.writerow([original_sentence, neighbor_sentence, f\"{similarity_score:.4f}\"])\n",
    "\n",
    "        # --- 4. Find Nearest Neighbors in the Test Set ---\n",
    "        print(\"\\n\\n--- Processing Test Set ---\")\n",
    "        nn_model_test = NearestNeighbors(n_neighbors=2, metric='cosine', algorithm='brute')\n",
    "        nn_model_test.fit(test_vectors)\n",
    "        distances_test, indices_test = nn_model_test.kneighbors(test_vectors)\n",
    "\n",
    "        output_test_filename = \"test_nearest_neighbors.csv\"\n",
    "        with open(output_test_filename, 'w', encoding='utf-8', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['original_sentence', 'nearest_neighbor', 'cosine_similarity'])\n",
    "            \n",
    "            print(f\"Saving all test neighbors to {output_test_filename}...\")\n",
    "            for i in tqdm(range(len(test_sentences)), desc=\"Test Neighbors\"):\n",
    "                original_sentence = test_sentences[i]\n",
    "                neighbor_index = indices_test[i][1]\n",
    "                neighbor_sentence = test_sentences[neighbor_index]\n",
    "                similarity_score = 1 - distances_test[i][1]\n",
    "                writer.writerow([original_sentence, neighbor_sentence, f\"{similarity_score:.4f}\"])\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nError: {e}.\")\n",
    "        print(\"Please make sure you have successfully run the 'tfidf_vectorizer.py' script first,\")\n",
    "        print(\"and that the .npz and .txt files are in the same directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
