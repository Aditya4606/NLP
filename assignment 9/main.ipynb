{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c2bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading first 10000 lines from /Users/adityakumar/Desktop/college_labs/NLP/assignment 9/gu.txt...\n",
      "Loaded 10000 lines.\n",
      "\n",
      "==============================\n",
      "--- Starting BPE Training ---\n",
      "Target merges: 1000\n",
      "==============================\n",
      "BPE: Getting word counts...\n",
      "BPE: Initial vocab size: 195\n",
      "BPE: Merge step 1/1000 - Best pair: ('рк╛', '</w>') -> рк╛</w>\n",
      "BPE: Merge step 100/1000 - Best pair: ('ркЕ', '</w>') -> ркЕ</w>\n",
      "BPE: Merge step 200/1000 - Best pair: ('ркШ', 'рк░</w>') -> ркШрк░</w>\n",
      "BPE: Merge step 300/1000 - Best pair: ('ркП', 'рк╕') -> ркПрк╕\n",
      "BPE: Merge step 400/1000 - Best pair: ('ркЙ', 'рк▓</w>') -> ркЙрк▓</w>\n",
      "BPE: Merge step 500/1000 - Best pair: ('ркЯ', 'рк▓рко</w>') -> ркЯрк▓рко</w>\n",
      "BPE: Merge step 600/1000 - Best pair: ('рк╖', 'рке</w>') -> рк╖рке</w>\n",
      "BPE: Merge step 700/1000 - Best pair: ('ркП', 'ркХрко</w>') -> ркПркХрко</w>\n",
      "BPE: Merge step 800/1000 - Best pair: ('1', '8') -> 18\n",
      "BPE: Merge step 900/1000 - Best pair: ('8', '0</w>') -> 80</w>\n",
      "BPE: Merge step 1000/1000 - Best pair: ('i', 'n</w>') -> in</w>\n",
      "BPE: Training complete.\n",
      "BPE training took 11.17 seconds.\n",
      "\n",
      "==============================\n",
      "--- Starting WordPiece Training ---\n",
      "Target vocab size: 1000\n",
      "==============================\n",
      "WordPiece: Getting word counts...\n",
      "WordPiece: Initial vocab size: 199\n",
      "WordPiece: Will perform 801 merges.\n",
      "WordPiece: Merge 1/801 - Vocab size: 200 - Best pair: ('ро┤', 'ро░') -> ро┤ро░\n",
      "WordPiece: Merge 100/801 - Vocab size: 299 - Best pair: ('RT', 'L') -> RTL\n",
      "WordPiece: Merge 200/801 - Vocab size: 399 - Best pair: ('R', 'SCHHORN') -> RSCHHORN\n",
      "WordPiece: Merge 300/801 - Vocab size: 499 - Best pair: ('H', 'PO') -> HPO\n",
      "WordPiece: Merge 400/801 - Vocab size: 599 - Best pair: ('Dzhank', 'a') -> Dzhanka\n",
      "WordPiece: Merge 500/801 - Vocab size: 699 - Best pair: ('Divy', 'a') -> Divya\n",
      "WordPiece: Merge 600/801 - Vocab size: 799 - Best pair: ('cov', 'e') -> cove\n",
      "WordPiece: Merge 700/801 - Vocab size: 899 - Best pair: ('CC', 'I') -> CCI\n",
      "WordPiece: Merge 800/801 - Vocab size: 999 - Best pair: ('C', 'SS') -> CSS\n",
      "WordPiece: Training complete.\n",
      "WordPiece training took 12.05 seconds.\n",
      "\n",
      "Test Sentence: ркЧрлБркЬрк░рк╛ркдркирлБркВ рк╕рлМркерлА ркорлЛркЯрлБркВ рк╢рк╣рлЗрк░ ркЕркоркжрк╛рк╡рк╛ркж ркЫрлЗ.\n",
      "\n",
      "--- BPE Tokenization Test ---\n",
      "BPE Tokens: ['ркЧ</w>', 'рлБ</w>', 'ркЬрк░</w>', 'рк╛</w>', 'ркдрки</w>', 'рлБ</w>', 'ркВ</w>', 'рк╕</w>', 'рлМ</w>', 'рке</w>', 'рлА</w>', 'рко</w>', 'рлЛ</w>', 'ркЯ</w>', 'рлБ</w>', 'ркВ</w>', 'рк╢рк╣</w>', 'рлЗ</w>', 'рк░</w>', 'ркЕркоркж</w>', 'рк╛</w>', 'рк╡</w>', 'рк╛</w>', 'ркж</w>', 'ркЫ</w>', 'рлЗ</w>', '.</w>']\n",
      "\n",
      "--- WordPiece Tokenization Test ---\n",
      "WordPiece Tokens: ['ркЧ', 'рлБ', 'ркЬ', '##рк░', 'рк╛', 'ркд', '##рки', 'рлБ', 'ркВ', 'рк╕', 'рлМ', 'рке', 'рлА', 'рко', 'рлЛ', 'ркЯ', 'рлБ', 'ркВ', 'рк╢', '##рк╣', 'рлЗ', 'рк░', 'ркЕ', '##рко', '##ркж', 'рк╛', 'рк╡', 'рк╛', 'ркж', 'ркЫ', 'рлЗ', '.']\n",
      "\n",
      "Saving vocab files...\n",
      "Saved 'bpe_vocab.txt', 'bpe_merges.txt', and 'wordpiece_vocab.txt'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "import time\n",
    "\n",
    "\n",
    "LOCAL_DATA_FILE_PATH = \"/Users/adityakumar/Desktop/college_labs/NLP/assignment 9/gu.txt\" \n",
    "\n",
    "# We'll use 10,000 samples and 1,000 merges/vocab size for a quick test.\n",
    "NUM_SAMPLES_FOR_TRAINING = 10000\n",
    "NUM_MERGES_BPE = 1000\n",
    "TARGET_VOCAB_SIZE_WP = 1000\n",
    "# -----------------------------------\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. BYTE PAIR ENCODING (BPE)\n",
    "# =============================================================================\n",
    "\n",
    "def get_word_counts_bpe(corpus):\n",
    "    \"\"\"\n",
    "    Pre-tokenizes a raw text corpus into a dictionary of word counts.\n",
    "    Adds a space between characters and an end-of-word token.\n",
    "    \"\"\"\n",
    "    word_counts = collections.Counter()\n",
    "    for text in corpus:\n",
    "        # This regex handles words (including Indic scripts) and punctuation\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "        for word in words:\n",
    "            # Add the end-of-word token </w>\n",
    "            word_counts[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return word_counts\n",
    "\n",
    "def get_pairs_bpe(word_counts):\n",
    "    \"\"\"\n",
    "    Finds the frequency of all adjacent token pairs in the vocabulary.\n",
    "    \"\"\"\n",
    "    pairs = collections.Counter()\n",
    "    for word, count in word_counts.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += count\n",
    "    return pairs\n",
    "\n",
    "def merge_pair_bpe(best_pair, word_counts):\n",
    "    \"\"\"\n",
    "    Merges the most frequent pair in all words of the vocabulary.\n",
    "    \"\"\"\n",
    "    new_word_counts = collections.Counter()\n",
    "    new_token = ''.join(best_pair)\n",
    "    \n",
    "    # We need a regex to safely replace the pair\n",
    "    # (?<!\\S) and (?!\\S) are negative look-behind/ahead for whitespace\n",
    "    # This ensures we only merge whole tokens.\n",
    "    pattern = r'(?<!\\S)' + re.escape(best_pair[0]) + r'\\s+' + re.escape(best_pair[1]) + r'(?!\\S)'\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        # Replace all occurrences of the pair with the new merged token\n",
    "        new_word = re.sub(pattern, new_token, word)\n",
    "        new_word_counts[new_word] += count\n",
    "        \n",
    "    return new_word_counts\n",
    "\n",
    "def train_bpe(corpus, num_merges):\n",
    "    \"\"\"\n",
    "    Trains a BPE tokenizer from a corpus.\n",
    "    \"\"\"\n",
    "    print(\"BPE: Getting word counts...\")\n",
    "    # 1. Get word frequencies\n",
    "    word_counts = get_word_counts_bpe(corpus)\n",
    "    \n",
    "    # 2. Get initial vocabulary (all unique characters)\n",
    "    vocab = set()\n",
    "    for word in word_counts:\n",
    "        vocab.update(word.split())\n",
    "    print(f\"BPE: Initial vocab size: {len(vocab)}\")\n",
    "        \n",
    "    merge_rules = []\n",
    "    \n",
    "    # 3. Iterate for num_merges\n",
    "    for i in range(num_merges):\n",
    "        # 3a. Get all adjacent pairs\n",
    "        pairs = get_pairs_bpe(word_counts)\n",
    "        \n",
    "        if not pairs:\n",
    "            print(\"BPE: No more pairs to merge.\")\n",
    "            break\n",
    "            \n",
    "        # 3b. Find the most frequent pair\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        \n",
    "        # 3c. Merge the pair and add to vocab\n",
    "        new_token = ''.join(best_pair)\n",
    "        vocab.add(new_token)\n",
    "        merge_rules.append(best_pair)\n",
    "        \n",
    "        # 3d. Update all word representations\n",
    "        word_counts = merge_pair_bpe(best_pair, word_counts)\n",
    "        \n",
    "        if (i + 1) % 100 == 0 or i == 0:\n",
    "            print(f\"BPE: Merge step {i+1}/{num_merges} - Best pair: {best_pair} -> {new_token}\")\n",
    "\n",
    "    print(\"BPE: Training complete.\")\n",
    "    return vocab, merge_rules\n",
    "\n",
    "def tokenize_bpe(text, merge_rules):\n",
    "    \"\"\"\n",
    "    Tokenizes new text using the learned BPE merge rules.\n",
    "    \"\"\"\n",
    "    # 1. Pre-tokenize into words (using the same regex as training)\n",
    "    words = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "    \n",
    "    tokenized_output = []\n",
    "    for word in words:\n",
    "        # 2. Represent word as chars + </w>\n",
    "        tokens = list(word) + ['</w>']\n",
    "        \n",
    "        # 3. Apply all merge rules in order\n",
    "        for pair in merge_rules:\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                # Check if the current and next token form the pair\n",
    "                if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:\n",
    "                    new_tokens.append(''.join(pair))\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            tokens = new_tokens\n",
    "            \n",
    "        tokenized_output.extend(tokens)\n",
    "        \n",
    "    return tokenized_output\n",
    "\n",
    "# =============================================================================\n",
    "# 2. WORDPIECE\n",
    "# =============================================================================\n",
    "\n",
    "def get_word_counts_wp(corpus):\n",
    "    \"\"\"\n",
    "    Pre-tokenizes corpus into word counts.\n",
    "    WordPiece splits words into characters.\n",
    "    \"\"\"\n",
    "    word_counts = collections.Counter()\n",
    "    for text in corpus:\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "        for word in words:\n",
    "            # We just split into characters. No '</w>'\n",
    "            word_counts[' '.join(list(word))] += 1\n",
    "    return word_counts\n",
    "\n",
    "def get_stats_wp(word_counts):\n",
    "    \"\"\"\n",
    "    Gets counts for both individual tokens and adjacent pairs.\n",
    "    \"\"\"\n",
    "    pairs = collections.Counter()\n",
    "    token_counts = collections.Counter()\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)):\n",
    "            token_counts[symbols[i]] += count\n",
    "            if i < len(symbols) - 1:\n",
    "                pairs[(symbols[i], symbols[i+1])] += count\n",
    "    return pairs, token_counts\n",
    "\n",
    "def merge_pair_wp(best_pair, word_counts):\n",
    "    \"\"\"\n",
    "    Merges the best pair in all words (same as BPE's merge).\n",
    "    \"\"\"\n",
    "    new_word_counts = collections.Counter()\n",
    "    new_token = ''.join(best_pair)\n",
    "    \n",
    "    pattern = r'(?<!\\S)' + re.escape(best_pair[0]) + r'\\s+' + re.escape(best_pair[1]) + r'(?!\\S)'\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        new_word = re.sub(pattern, new_token, word)\n",
    "        new_word_counts[new_word] += count\n",
    "        \n",
    "    return new_word_counts\n",
    "\n",
    "def train_wordpiece(corpus, vocab_size):\n",
    "    \"\"\"\n",
    "    Trains a WordPiece tokenizer from a corpus.\n",
    "    \"\"\"\n",
    "    print(\"WordPiece: Getting word counts...\")\n",
    "    # 1. Get word frequencies\n",
    "    word_counts = get_word_counts_wp(corpus)\n",
    "    \n",
    "    # 2. Get initial vocabulary (all unique characters)\n",
    "    vocab = set()\n",
    "    for word in word_counts:\n",
    "        vocab.update(word.split())\n",
    "    \n",
    "    # Add special tokens\n",
    "    special_tokens = ['[UNK]', '[CLS]', '[SEP]', '[PAD]', '[MASK]']\n",
    "    for token in special_tokens:\n",
    "        if token not in vocab:\n",
    "            vocab.add(token)\n",
    "            \n",
    "    print(f\"WordPiece: Initial vocab size: {len(vocab)}\")\n",
    "    \n",
    "    # We iterate until we reach the target vocab size\n",
    "    num_merges = vocab_size - len(vocab)\n",
    "    if num_merges <= 0:\n",
    "        print(f\"WordPiece: Target vocab size ({vocab_size}) is smaller than initial vocab size ({len(vocab)}). No merges needed.\")\n",
    "        return vocab\n",
    "    \n",
    "    print(f\"WordPiece: Will perform {num_merges} merges.\")\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        # 3a. Get pair and individual token counts\n",
    "        pairs, token_counts = get_stats_wp(word_counts)\n",
    "        \n",
    "        if not pairs:\n",
    "            print(\"WordPiece: No more pairs to merge.\")\n",
    "            break\n",
    "\n",
    "        # 3b. Find the best pair based on likelihood score\n",
    "        best_pair = ('', '')\n",
    "        max_score = -1.0\n",
    "        \n",
    "        for pair, count in pairs.items():\n",
    "            token_a, token_b = pair\n",
    "            # Ensure we don't divide by zero\n",
    "            if token_counts[token_a] > 0 and token_counts[token_b] > 0:\n",
    "                # Score = freq(pair) / (freq(A) * freq(B))\n",
    "                score = count / (token_counts[token_a] * token_counts[token_b])\n",
    "            else:\n",
    "                score = 0.0\n",
    "            \n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                best_pair = pair\n",
    "        \n",
    "        if max_score == -1.0:\n",
    "            print(\"WordPiece: No valid pairs found (max_score = -1).\")\n",
    "            break\n",
    "\n",
    "        # 3c. Merge the pair and add to vocab\n",
    "        new_token = ''.join(best_pair)\n",
    "        vocab.add(new_token)\n",
    "        \n",
    "        # 3d. Update all word representations\n",
    "        word_counts = merge_pair_wp(best_pair, word_counts)\n",
    "        \n",
    "        if (i + 1) % 100 == 0 or i == 0:\n",
    "            print(f\"WordPiece: Merge {i+1}/{num_merges} - Vocab size: {len(vocab)} - Best pair: {best_pair} -> {new_token}\")\n",
    "            \n",
    "    print(\"WordPiece: Training complete.\")\n",
    "    return vocab\n",
    "\n",
    "def tokenize_wordpiece(text, vocab, unk_token=\"[UNK]\"):\n",
    "    \"\"\"\n",
    "    Tokenizes new text using the learned WordPiece vocabulary.\n",
    "    Uses a greedy, longest-match-first approach.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Pre-tokenize into words\n",
    "    words = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "    \n",
    "    output_tokens = []\n",
    "    \n",
    "    for word in words:\n",
    "        # If the whole word is in our vocab, great!\n",
    "        if word in vocab:\n",
    "            output_tokens.append(word)\n",
    "            continue\n",
    "            \n",
    "        word_tokens = []\n",
    "        start = 0\n",
    "        while start < len(word):\n",
    "            # Find the longest subword in vocab that matches from `start`\n",
    "            end = len(word)\n",
    "            best_subword = None\n",
    "            \n",
    "            # Greedily check from longest possible (end) to shortest (start+1)\n",
    "            while end > start:\n",
    "                sub = word[start:end]\n",
    "                # Note: WordPiece vocab doesn't store '##' prefixes\n",
    "                if sub in vocab:\n",
    "                    best_subword = sub\n",
    "                    break # Found the longest valid subword\n",
    "                end -= 1\n",
    "            \n",
    "            # Case 1: No subword found (not even a single character)\n",
    "            if best_subword is None:\n",
    "                # This means the character at word[start] is not in our vocab\n",
    "                # This is a true \"unknown\"\n",
    "                word_tokens = [unk_token] # The whole word is un-tokenizable\n",
    "                break # Stop processing this word\n",
    "            \n",
    "            # Case 2: We found a subword\n",
    "            if start == 0:\n",
    "                # It's the first piece of the word\n",
    "                word_tokens.append(best_subword)\n",
    "            else:\n",
    "                # It's a subsequent piece, add the '##' prefix\n",
    "                word_tokens.append(\"##\" + best_subword)\n",
    "            \n",
    "            # Move our `start` pointer to the end of the subword we just found\n",
    "            start += len(best_subword)\n",
    "        \n",
    "        output_tokens.extend(word_tokens)\n",
    "        \n",
    "    return output_tokens\n",
    "\n",
    "# =============================================================================\n",
    "# 3. MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- 1. Load Data ---\n",
    "    print(f\"Loading first {NUM_SAMPLES_FOR_TRAINING} lines from {LOCAL_DATA_FILE_PATH}...\")\n",
    "    corpus = []\n",
    "    try:\n",
    "        with open(LOCAL_DATA_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= NUM_SAMPLES_FOR_TRAINING:\n",
    "                    break\n",
    "                # .strip() removes leading/trailing whitespace and newlines\n",
    "                corpus.append(line.strip())\n",
    "                \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{LOCAL_DATA_FILE_PATH}' was not found.\")\n",
    "        print(\"Please update the LOCAL_DATA_FILE_PATH variable at the top of the script.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Loaded {len(corpus)} lines.\")\n",
    "    if not corpus:\n",
    "        print(\"Corpus is empty! Check your file and file path. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Train BPE ---\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"--- Starting BPE Training ---\")\n",
    "    print(f\"Target merges: {NUM_MERGES_BPE}\")\n",
    "    print(\"=\"*30)\n",
    "    start_time = time.time()\n",
    "    bpe_vocab, bpe_rules = train_bpe(corpus, NUM_MERGES_BPE)\n",
    "    end_time = time.time()\n",
    "    print(f\"BPE training took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    # --- 3. Train WordPiece ---\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"--- Starting WordPiece Training ---\")\n",
    "    print(f\"Target vocab size: {TARGET_VOCAB_SIZE_WP}\")\n",
    "    print(\"=\"*30)\n",
    "    start_time = time.time()\n",
    "    wp_vocab = train_wordpiece(corpus, TARGET_VOCAB_SIZE_WP)\n",
    "    end_time = time.time()\n",
    "    print(f\"WordPiece training took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "    # --- 4. Test Tokenization ---\n",
    "    test_sentence = \"ркЧрлБркЬрк░рк╛ркдркирлБркВ рк╕рлМркерлА ркорлЛркЯрлБркВ рк╢рк╣рлЗрк░ ркЕркоркжрк╛рк╡рк╛ркж ркЫрлЗ.\" # \"Ahmedabad is the largest city in Gujarat.\"\n",
    "    print(f\"\\nTest Sentence: {test_sentence}\")\n",
    "\n",
    "    # Test BPE\n",
    "    print(\"\\n--- BPE Tokenization Test ---\")\n",
    "    bpe_tokens = tokenize_bpe(test_sentence, bpe_rules)\n",
    "    print(f\"BPE Tokens: {bpe_tokens}\")\n",
    "\n",
    "    # Test WordPiece\n",
    "    print(\"\\n--- WordPiece Tokenization Test ---\")\n",
    "    wp_tokens = tokenize_wordpiece(test_sentence, wp_vocab)\n",
    "    print(f\"WordPiece Tokens: {wp_tokens}\")\n",
    "\n",
    "    # --- 5. Save Vocabularies (Optional but Recommended) ---\n",
    "    print(\"\\nSaving vocab files...\")\n",
    "    \n",
    "    # Save BPE vocab and rules\n",
    "    with open(\"bpe_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for token in sorted(list(bpe_vocab)):\n",
    "            f.write(token + \"\\n\")\n",
    "            \n",
    "    with open(\"bpe_merges.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for pair in bpe_rules:\n",
    "            f.write(f\"{pair[0]} {pair[1]}\\n\")\n",
    "\n",
    "    # Save WordPiece vocab\n",
    "    with open(\"wordpiece_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        # WordPiece vocabs are often sorted by length, then alphabetically\n",
    "        # But for this from-scratch version, alphabetical is fine.\n",
    "        for token in sorted(list(wp_vocab)):\n",
    "            f.write(token + \"\\n\")\n",
    "            \n",
    "    print(\"Saved 'bpe_vocab.txt', 'bpe_merges.txt', and 'wordpiece_vocab.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b5458",
   "metadata": {},
   "source": [
    "<h1>on 1lakh sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10e894b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading first 100000 lines from /Users/adityakumar/Desktop/college_labs/NLP/assignment 9/gu.txt...\n",
      "Loaded 100000 lines.\n",
      "\n",
      "==============================\n",
      "--- Starting BPE Training ---\n",
      "Target merges: 32000\n",
      "==============================\n",
      "BPE: Getting word counts...\n",
      "BPE: Initial vocab size: 339\n",
      "BPE: Merge step 1/32000 - Best pair: ('рк╛', '</w>') -> рк╛</w>\n",
      "BPE: Merge step 500/32000 - Best pair: ('рк╕рко', 'ркпрко</w>') -> рк╕ркоркпрко</w>\n",
      "BPE: Merge step 1000/32000 - Best pair: ('ркн', 'ркЯ</w>') -> ркнркЯ</w>\n",
      "BPE: Merge step 1500/32000 - Best pair: ('ркЖ', 'ркЬрк░</w>') -> ркЖркЬрк░</w>\n",
      "BPE: Merge step 2000/32000 - Best pair: ('рк╡', 'ркиркЧрк░рки</w>') -> рк╡ркиркЧрк░рки</w>\n",
      "BPE: Merge step 2500/32000 - Best pair: ('ркЯ', 'рк╕рки</w>') -> ркЯрк╕рки</w>\n",
      "BPE: Merge step 3000/32000 - Best pair: ('ркп', 'ркХрк╡</w>') -> ркпркХрк╡</w>\n",
      "BPE: Merge step 3500/32000 - Best pair: ('en', 'd') -> end\n",
      "BPE: Merge step 4000/32000 - Best pair: ('ркЕркЧ', 'рк╡ркбркд</w>') -> ркЕркЧрк╡ркбркд</w>\n",
      "BPE: Merge step 4500/32000 - Best pair: ('3', '60</w>') -> 360</w>\n",
      "BPE: Merge step 5000/32000 - Best pair: ('in', 'ter') -> inter\n",
      "BPE: Merge step 5500/32000 - Best pair: ('ркЖркЧ', 'ркоркирке</w>') -> ркЖркЧркоркирке</w>\n",
      "BPE: Merge step 6000/32000 - Best pair: ('рлк', 'рлжрлм</w>') -> рлкрлжрлм</w>\n",
      "BPE: Merge step 6500/32000 - Best pair: ('ркж', 'рк▓ркЦ</w>') -> ркжрк▓ркЦ</w>\n",
      "BPE: Merge step 7000/32000 - Best pair: ('ркзркм', 'ркбркХ</w>') -> ркзркмркбркХ</w>\n",
      "BPE: Merge step 7500/32000 - Best pair: ('ov', 'o</w>') -> ovo</w>\n",
      "BPE: Merge step 8000/32000 - Best pair: ('ркХ', '2рк╡</w>') -> ркХ2рк╡</w>\n",
      "BPE: Merge step 8500/32000 - Best pair: ('2', '17</w>') -> 217</w>\n",
      "BPE: Merge step 9000/32000 - Best pair: ('рлзрлн', 'рлл') -> рлзрлнрлл\n",
      "BPE: Merge step 9500/32000 - Best pair: ('IP', 'рки</w>') -> IPрки</w>\n",
      "BPE: Merge step 10000/32000 - Best pair: ('GD', 'Pрко</w>') -> GDPрко</w>\n",
      "BPE: Merge step 10500/32000 - Best pair: ('NSAID', 's</w>') -> NSAIDs</w>\n",
      "BPE: Merge step 11000/32000 - Best pair: ('рли', 'рлирло</w>') -> рлирлирло</w>\n",
      "BPE: Merge step 11500/32000 - Best pair: ('19', '40</w>') -> 1940</w>\n",
      "BPE: Merge step 12000/32000 - Best pair: ('рк╣рк╕', 'ркирккрк░</w>') -> рк╣рк╕ркирккрк░</w>\n",
      "BPE: Merge step 12500/32000 - Best pair: ('Moise', 'evich</w>') -> Moiseevich</w>\n",
      "BPE: Merge step 13000/32000 - Best pair: ('рк╕', 'рк░ркм</w>') -> рк╕рк░ркм</w>\n",
      "BPE: Merge step 13500/32000 - Best pair: ('45', '2') -> 452\n",
      "BPE: Merge step 14000/32000 - Best pair: ('рк░', 'ркЖ</w>') -> рк░ркЖ</w>\n",
      "BPE: Merge step 14500/32000 - Best pair: ('рк░ркд', 'ркиркм</w>') -> рк░ркдркиркм</w>\n",
      "BPE: Merge step 15000/32000 - Best pair: ('ркЯ', 'рк▓рк░рк╢</w>') -> ркЯрк▓рк░рк╢</w>\n",
      "BPE: Merge step 15500/32000 - Best pair: ('P', 'ila</w>') -> Pila</w>\n",
      "BPE: Merge step 16000/32000 - Best pair: ('рк│ркЦ', 'ркЧркд</w>') -> рк│ркЦркЧркд</w>\n",
      "BPE: Merge step 16500/32000 - Best pair: ('Glycol', 'рки</w>') -> Glycolрки</w>\n",
      "BPE: Merge step 17000/32000 - Best pair: ('Dizz', 'iness</w>') -> Dizziness</w>\n",
      "BPE: Merge step 17500/32000 - Best pair: ('рлк', 'рлзрлн</w>') -> рлкрлзрлн</w>\n",
      "BPE: Merge step 18000/32000 - Best pair: ('ркд', 'ркбрки</w>') -> ркдркбрки</w>\n",
      "BPE: Merge step 18500/32000 - Best pair: ('over', 'loo') -> overloo\n",
      "BPE: Merge step 19000/32000 - Best pair: ('рк╣', 'ркЪркб</w>') -> рк╣ркЪркб</w>\n",
      "BPE: Merge step 19500/32000 - Best pair: ('рлзрлкрлирло', 'рлжрки</w>') -> рлзрлкрлирлорлжрки</w>\n",
      "BPE: Merge step 20000/32000 - Best pair: ('9', '00рки</w>') -> 900рки</w>\n",
      "BPE: Merge step 20500/32000 - Best pair: ('рлкрлжрлж', 'рлжрлж') -> рлкрлжрлжрлжрлж\n",
      "BPE: Merge step 21000/32000 - Best pair: ('рк╡', 'рккрк▓</w>') -> рк╡рккрк▓</w>\n",
      "BPE: Merge step 21500/32000 - Best pair: ('NM', 'MC</w>') -> NMMC</w>\n",
      "BPE: Merge step 22000/32000 - Best pair: ('рллрлм', 'рлзрлн</w>') -> рллрлмрлзрлн</w>\n",
      "BPE: Merge step 22500/32000 - Best pair: ('AB', 'Z') -> ABZ\n",
      "BPE: Merge step 23000/32000 - Best pair: ('рлз', 'рлзрлйрлжрлж</w>') -> рлзрлзрлйрлжрлж</w>\n",
      "BPE: Merge step 23500/32000 - Best pair: ('рлм', 'рлзрлпрлйрлж</w>') -> рлмрлзрлпрлйрлж</w>\n",
      "BPE: Merge step 24000/32000 - Best pair: ('SU', 'UT') -> SUUT\n",
      "BPE: Merge step 24500/32000 - Best pair: ('sn', 'ow') -> snow\n",
      "BPE: Merge step 25000/32000 - Best pair: ('G', 'ugg') -> Gugg\n",
      "BPE: Merge step 25500/32000 - Best pair: ('Alflut', 'op</w>') -> Alflutop</w>\n",
      "BPE: Merge step 26000/32000 - Best pair: ('Decarbon', 'izer</w>') -> Decarbonizer</w>\n",
      "BPE: Merge step 26500/32000 - Best pair: ('Lim', 'pi') -> Limpi\n",
      "BPE: Merge step 27000/32000 - Best pair: ('рлзрлм', 'рллрки</w>') -> рлзрлмрллрки</w>\n",
      "BPE: Merge step 27500/32000 - Best pair: ('AMD', 'AB') -> AMDAB\n",
      "BPE: Merge step 28000/32000 - Best pair: ('Indi', 'ra</w>') -> Indira</w>\n",
      "BPE: Merge step 28500/32000 - Best pair: ('33', '7рки</w>') -> 337рки</w>\n",
      "BPE: Merge step 29000/32000 - Best pair: ('Aluthgam', 'age</w>') -> Aluthgamage</w>\n",
      "BPE: Merge step 29500/32000 - Best pair: ('ркЫ', 'ркХрккрк░</w>') -> ркЫркХрккрк░</w>\n",
      "BPE: Merge step 30000/32000 - Best pair: ('917', '35') -> 91735\n",
      "BPE: Merge step 30500/32000 - Best pair: ('ркХркЯ', 'рк░ркж</w>') -> ркХркЯрк░ркж</w>\n",
      "BPE: Merge step 31000/32000 - Best pair: ('ркп', 'ркЙркЬрк╡ркг</w>') -> ркпркЙркЬрк╡ркг</w>\n",
      "BPE: Merge step 31500/32000 - Best pair: ('f', 'ill</w>') -> fill</w>\n",
      "BPE: Merge step 32000/32000 - Best pair: ('Re', 'ti') -> Reti\n",
      "BPE: Training complete.\n",
      "BPE training took 941.82 seconds.\n",
      "\n",
      "==============================\n",
      "--- Starting WordPiece Training ---\n",
      "Target vocab size: 32000\n",
      "==============================\n",
      "WordPiece: Getting word counts...\n",
      "WordPiece: Initial vocab size: 343\n",
      "WordPiece: Will perform 31657 merges.\n",
      "WordPiece: Merge 1/31657 - Vocab size: 344 - Best pair: ('ро┤', 'ро░') -> ро┤ро░\n",
      "WordPiece: Merge 500/31657 - Vocab size: 843 - Best pair: ('e', 'arthquake') -> earthquake\n",
      "WordPiece: Merge 1000/31657 - Vocab size: 1343 - Best pair: ('requirement', 's') -> requirements\n",
      "WordPiece: Merge 1500/31657 - Vocab size: 1843 - Best pair: ('n', 'utsyapomozhet') -> nutsyapomozhet\n",
      "WordPiece: Merge 2000/31657 - Vocab size: 2343 - Best pair: ('Vijay', 'v') -> Vijayv\n",
      "WordPiece: Merge 2500/31657 - Vocab size: 2843 - Best pair: ('lovz', 'o') -> lovzo\n",
      "WordPiece: Merge 3000/31657 - Vocab size: 3343 - Best pair: ('Uptoda', 't') -> Uptodat\n",
      "WordPiece: Merge 3500/31657 - Vocab size: 3843 - Best pair: ('c', 'ontraindic') -> contraindic\n",
      "WordPiece: Merge 4000/31657 - Vocab size: 4343 - Best pair: ('DHC', 'P') -> DHCP\n",
      "WordPiece: Merge 4500/31657 - Vocab size: 4843 - Best pair: ('CS', 'K') -> CSK\n",
      "WordPiece: Merge 5000/31657 - Vocab size: 5343 - Best pair: ('K', 'ASH') -> KASH\n",
      "WordPiece: Merge 5500/31657 - Vocab size: 5843 - Best pair: ('a', 'lling') -> alling\n",
      "WordPiece: Merge 6000/31657 - Vocab size: 6343 - Best pair: ('Liv', 'e') -> Live\n",
      "WordPiece: Merge 6500/31657 - Vocab size: 6843 - Best pair: ('hy', 'a') -> hya\n",
      "WordPiece: Merge 7000/31657 - Vocab size: 7343 - Best pair: ('subs', 't') -> subst\n",
      "WordPiece: Merge 7500/31657 - Vocab size: 7843 - Best pair: ('Memo', 'r') -> Memor\n",
      "WordPiece: Merge 8000/31657 - Vocab size: 8343 - Best pair: ('o', 'yuz') -> oyuz\n",
      "WordPiece: Merge 8500/31657 - Vocab size: 8843 - Best pair: ('eversio', 'n') -> eversion\n",
      "WordPiece: Merge 9000/31657 - Vocab size: 9343 - Best pair: ('chhotaudep', 'u') -> chhotaudepu\n",
      "WordPiece: Merge 9500/31657 - Vocab size: 9843 - Best pair: ('washe', 'd') -> washed\n",
      "WordPiece: Merge 10000/31657 - Vocab size: 10343 - Best pair: ('t', 'apych') -> tapych\n",
      "WordPiece: Merge 10500/31657 - Vocab size: 10843 - Best pair: ('S', 'anand') -> Sanand\n",
      "WordPiece: Merge 11000/31657 - Vocab size: 11343 - Best pair: ('pic', 's') -> pics\n",
      "WordPiece: Merge 11500/31657 - Vocab size: 11843 - Best pair: ('s', 'unni') -> sunni\n",
      "WordPiece: Merge 12000/31657 - Vocab size: 12343 - Best pair: ('shek', 'e') -> sheke\n",
      "WordPiece: Merge 12500/31657 - Vocab size: 12843 - Best pair: ('VKontak', 't') -> VKontakt\n",
      "WordPiece: Merge 13000/31657 - Vocab size: 13343 - Best pair: ('o', 'lyn') -> olyn\n",
      "WordPiece: Merge 13500/31657 - Vocab size: 13843 - Best pair: ('a', 'spoons') -> aspoons\n",
      "WordPiece: Merge 14000/31657 - Vocab size: 14343 - Best pair: ('m', 'atologists') -> matologists\n",
      "WordPiece: Merge 14500/31657 - Vocab size: 14843 - Best pair: ('f', 'ed') -> fed\n",
      "WordPiece: Merge 15000/31657 - Vocab size: 15343 - Best pair: ('t', 'v') -> tv\n",
      "WordPiece: Merge 15500/31657 - Vocab size: 15843 - Best pair: ('interes', 't') -> interest\n",
      "WordPiece: Merge 16000/31657 - Vocab size: 16343 - Best pair: ('cleom', 'e') -> cleome\n",
      "WordPiece: Merge 16500/31657 - Vocab size: 16843 - Best pair: ('dee', 'p') -> deep\n",
      "WordPiece: Merge 17000/31657 - Vocab size: 17343 - Best pair: ('B', 'tp') -> Btp\n",
      "WordPiece: Merge 17500/31657 - Vocab size: 17843 - Best pair: ('2', '56GB') -> 256GB\n",
      "WordPiece: Merge 18000/31657 - Vocab size: 18343 - Best pair: ('рлйрлл', 'рлнрлл') -> рлйрллрлнрлл\n",
      "WordPiece: Merge 18500/31657 - Vocab size: 18843 - Best pair: ('рлнрлйрло', 'рлк') -> рлнрлйрлорлк\n",
      "WordPiece: Merge 19000/31657 - Vocab size: 19343 - Best pair: ('рлз', 'рлмрлл') -> рлзрлмрлл\n",
      "WordPiece: Merge 19500/31657 - Vocab size: 19843 - Best pair: ('720x', '1') -> 720x1\n",
      "WordPiece: Merge 20000/31657 - Vocab size: 20343 - Best pair: ('4', '69') -> 469\n",
      "WordPiece: Merge 20500/31657 - Vocab size: 20843 - Best pair: ('18', '66') -> 1866\n",
      "WordPiece: Merge 21000/31657 - Vocab size: 21343 - Best pair: ('23', '29') -> 2329\n",
      "WordPiece: Merge 21500/31657 - Vocab size: 21843 - Best pair: ('7016', '3') -> 70163\n",
      "WordPiece: Merge 22000/31657 - Vocab size: 22343 - Best pair: ('8', '192') -> 8192\n",
      "WordPiece: Merge 22500/31657 - Vocab size: 22843 - Best pair: ('рла', 'ркд') -> рларкд\n",
      "WordPiece: Merge 23000/31657 - Vocab size: 23343 - Best pair: ('BTP', 'рке') -> BTPрке\n",
      "WordPiece: Merge 23500/31657 - Vocab size: 23843 - Best pair: ('рк░рлжрлзрлм', 'рки') -> рк░рлжрлзрлмрки\n",
      "WordPiece: Merge 24000/31657 - Vocab size: 24343 - Best pair: ('рлкрлкрлж', 'рки') -> рлкрлкрлжрки\n",
      "WordPiece: Merge 24500/31657 - Vocab size: 24843 - Best pair: ('рлзрлмрлк', 'рко') -> рлзрлмрлкрко\n",
      "WordPiece: Merge 25000/31657 - Vocab size: 25343 - Best pair: ('ркЕркбркж', 'ркж') -> ркЕркбркжркж\n",
      "WordPiece: Merge 25500/31657 - Vocab size: 25843 - Best pair: ('ркПрк╕ркП', 'рки') -> ркПрк╕ркПрки\n",
      "WordPiece: Merge 26000/31657 - Vocab size: 26343 - Best pair: ('рк░ркоркгркн', 'ркоркг') -> рк░ркоркгркнркоркг\n",
      "WordPiece: Merge 26500/31657 - Vocab size: 26843 - Best pair: ('ркЗ', 'ркЗрк╕') -> ркЗркЗрк╕\n",
      "WordPiece: Merge 27000/31657 - Vocab size: 27343 - Best pair: ('20', 'рко') -> 20рко\n",
      "WordPiece: Merge 27500/31657 - Vocab size: 27843 - Best pair: ('ркЙркЯрк░', 'рки') -> ркЙркЯрк░рки\n",
      "WordPiece: Merge 28000/31657 - Vocab size: 28343 - Best pair: ('ркЙркгркк', 'рко') -> ркЙркгрккрко\n",
      "WordPiece: Merge 28500/31657 - Vocab size: 28843 - Best pair: ('ркЦ', 'рли') -> ркЦрли\n",
      "WordPiece: Merge 29000/31657 - Vocab size: 29343 - Best pair: ('рк▓', 'рккркдркн') -> рк▓рккркдркн\n",
      "WordPiece: Merge 29500/31657 - Vocab size: 29843 - Best pair: ('ркЬ', 'рк╕ркжркгрки') -> ркЬрк╕ркжркгрки\n",
      "WordPiece: Merge 30000/31657 - Vocab size: 30343 - Best pair: ('ркИ', 'рк░рк╕') -> ркИрк░рк╕\n",
      "WordPiece: Merge 30500/31657 - Vocab size: 30843 - Best pair: ('ркЙркд', 'рккрки') -> ркЙркдрккрки\n",
      "WordPiece: Merge 31000/31657 - Vocab size: 31343 - Best pair: ('рк░ркоркг', 'ркХ') -> рк░ркоркгркХ\n",
      "WordPiece: Merge 31500/31657 - Vocab size: 31843 - Best pair: ('ркЙркн', 'ркбркХ') -> ркЙркнркбркХ\n",
      "WordPiece: Training complete.\n",
      "WordPiece training took 1484.77 seconds.\n",
      "\n",
      "Test Sentence: ркЧрлБркЬрк░рк╛ркдркирлБркВ рк╕рлМркерлА ркорлЛркЯрлБркВ рк╢рк╣рлЗрк░ ркЕркоркжрк╛рк╡рк╛ркж ркЫрлЗ.\n",
      "\n",
      "--- BPE Tokenization Test ---\n",
      "BPE Tokens: ['ркЧ</w>', 'рлБ</w>', 'ркЬрк░</w>', 'рк╛</w>', 'ркдрки</w>', 'рлБ</w>', 'ркВ</w>', 'рк╕</w>', 'рлМ</w>', 'рке</w>', 'рлА</w>', 'рко</w>', 'рлЛ</w>', 'ркЯ</w>', 'рлБ</w>', 'ркВ</w>', 'рк╢рк╣</w>', 'рлЗ</w>', 'рк░</w>', 'ркЕркоркж</w>', 'рк╛</w>', 'рк╡</w>', 'рк╛</w>', 'ркж</w>', 'ркЫ</w>', 'рлЗ</w>', '.</w>']\n",
      "\n",
      "--- WordPiece Tokenization Test ---\n",
      "WordPiece Tokens: ['ркЧ', 'рлБ', 'ркЬрк░', 'рк╛', 'ркдрки', 'рлБ', 'ркВ', 'рк╕', 'рлМ', 'рке', 'рлА', 'рко', 'рлЛ', 'ркЯ', 'рлБ', 'ркВ', 'рк╢рк╣', 'рлЗ', 'рк░', 'ркЕркоркж', 'рк╛', 'рк╡', 'рк╛', 'ркж', 'ркЫ', 'рлЗ', '.']\n",
      "\n",
      "Saving vocab files for 32k run...\n",
      "Saved 'bpe_vocab_32k.txt', 'bpe_merges_32k.txt', and 'wordpiece_vocab_32k.txt'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "import time\n",
    "\n",
    "# --- тЪЩя╕П Settings ---\n",
    "# ЁЯСЗ **Double-check this path is correct**\n",
    "LOCAL_DATA_FILE_PATH = \"/Users/adityakumar/Desktop/college_labs/NLP/assignment 9/gu.txt\" \n",
    "\n",
    "# --- Settings for 1 Lakh (1 Hour) Run ---\n",
    "# This will run on 100,000 lines and perform 32,000 merges.\n",
    "# Expect it to take ~1 hour.\n",
    "NUM_SAMPLES_FOR_TRAINING = 100000  # 1 Lakh lines\n",
    "NUM_MERGES_BPE = 32000\n",
    "TARGET_VOCAB_SIZE_WP = 32000\n",
    "# -----------------------------------\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. BYTE PAIR ENCODING (BPE)\n",
    "# =============================================================================\n",
    "\n",
    "def get_word_counts_bpe(corpus):\n",
    "    \"\"\"\n",
    "    Pre-tokenizes a raw text corpus into a dictionary of word counts.\n",
    "    Adds a space between characters and an end-of-word token.\n",
    "    \"\"\"\n",
    "    word_counts = collections.Counter()\n",
    "    for text in corpus:\n",
    "        # This regex handles words (including Indic scripts) and punctuation\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "        for word in words:\n",
    "            # Add the end-of-word token </w>\n",
    "            word_counts[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return word_counts\n",
    "\n",
    "def get_pairs_bpe(word_counts):\n",
    "    \"\"\"\n",
    "    Finds the frequency of all adjacent token pairs in the vocabulary.\n",
    "    \"\"\"\n",
    "    pairs = collections.Counter()\n",
    "    for word, count in word_counts.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += count\n",
    "    return pairs\n",
    "\n",
    "def merge_pair_bpe(best_pair, word_counts):\n",
    "    \"\"\"\n",
    "    Merges the most frequent pair in all words of the vocabulary.\n",
    "    \"\"\"\n",
    "    new_word_counts = collections.Counter()\n",
    "    new_token = ''.join(best_pair)\n",
    "    \n",
    "    # We need a regex to safely replace the pair\n",
    "    # (?<!\\S) and (?!\\S) are negative look-behind/ahead for whitespace\n",
    "    # This ensures we only merge whole tokens.\n",
    "    pattern = r'(?<!\\S)' + re.escape(best_pair[0]) + r'\\s+' + re.escape(best_pair[1]) + r'(?!\\S)'\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        # Replace all occurrences of the pair with the new merged token\n",
    "        new_word = re.sub(pattern, new_token, word)\n",
    "        new_word_counts[new_word] += count\n",
    "        \n",
    "    return new_word_counts\n",
    "\n",
    "def train_bpe(corpus, num_merges):\n",
    "    \"\"\"\n",
    "    Trains a BPE tokenizer from a corpus.\n",
    "    \"\"\"\n",
    "    print(\"BPE: Getting word counts...\")\n",
    "    # 1. Get word frequencies\n",
    "    word_counts = get_word_counts_bpe(corpus)\n",
    "    \n",
    "    # 2. Get initial vocabulary (all unique characters)\n",
    "    vocab = set()\n",
    "    for word in word_counts:\n",
    "        vocab.update(word.split())\n",
    "    print(f\"BPE: Initial vocab size: {len(vocab)}\")\n",
    "        \n",
    "    merge_rules = []\n",
    "    \n",
    "    # 3. Iterate for num_merges\n",
    "    for i in range(num_merges):\n",
    "        # 3a. Get all adjacent pairs\n",
    "        pairs = get_pairs_bpe(word_counts)\n",
    "        \n",
    "        if not pairs:\n",
    "            print(f\"BPE: No more pairs to merge. Stopped at step {i+1}.\")\n",
    "            break\n",
    "            \n",
    "        # 3b. Find the most frequent pair\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        \n",
    "        # 3c. Merge the pair and add to vocab\n",
    "        new_token = ''.join(best_pair)\n",
    "        vocab.add(new_token)\n",
    "        merge_rules.append(best_pair)\n",
    "        \n",
    "        # 3d. Update all word representations\n",
    "        word_counts = merge_pair_bpe(best_pair, word_counts)\n",
    "        \n",
    "        if (i + 1) % 500 == 0 or i == 0:\n",
    "            print(f\"BPE: Merge step {i+1}/{num_merges} - Best pair: {best_pair} -> {new_token}\")\n",
    "\n",
    "    print(\"BPE: Training complete.\")\n",
    "    return vocab, merge_rules\n",
    "\n",
    "def tokenize_bpe(text, merge_rules):\n",
    "    \"\"\"\n",
    "    Tokenizes new text using the learned BPE merge rules.\n",
    "    \"\"\"\n",
    "    # 1. Pre-tokenize into words (using the same regex as training)\n",
    "    words = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "    \n",
    "    tokenized_output = []\n",
    "    for word in words:\n",
    "        # 2. Represent word as chars + </w>\n",
    "        tokens = list(word) + ['</w>']\n",
    "        \n",
    "        # 3. Apply all merge rules in order\n",
    "        for pair in merge_rules:\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                # Check if the current and next token form the pair\n",
    "                if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:\n",
    "                    new_tokens.append(''.join(pair))\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            tokens = new_tokens\n",
    "            \n",
    "        tokenized_output.extend(tokens)\n",
    "        \n",
    "    return tokenized_output\n",
    "\n",
    "# =============================================================================\n",
    "# 2. WORDPIECE\n",
    "# =============================================================================\n",
    "\n",
    "def get_word_counts_wp(corpus):\n",
    "    \"\"\"\n",
    "    Pre-tokenizes corpus into word counts.\n",
    "    WordPiece splits words into characters.\n",
    "    \"\"\"\n",
    "    word_counts = collections.Counter()\n",
    "    for text in corpus:\n",
    "        words = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "        for word in words:\n",
    "            # We just split into characters. No '</w>'\n",
    "            word_counts[' '.join(list(word))] += 1\n",
    "    return word_counts\n",
    "\n",
    "def get_stats_wp(word_counts):\n",
    "    \"\"\"\n",
    "    Gets counts for both individual tokens and adjacent pairs.\n",
    "    \"\"\"\n",
    "    pairs = collections.Counter()\n",
    "    token_counts = collections.Counter()\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)):\n",
    "            token_counts[symbols[i]] += count\n",
    "            if i < len(symbols) - 1:\n",
    "                pairs[(symbols[i], symbols[i+1])] += count\n",
    "    return pairs, token_counts\n",
    "\n",
    "def merge_pair_wp(best_pair, word_counts):\n",
    "    \"\"\"\n",
    "    Merges the best pair in all words (same as BPE's merge).\n",
    "    \"\"\"\n",
    "    new_word_counts = collections.Counter()\n",
    "    new_token = ''.join(best_pair)\n",
    "    \n",
    "    pattern = r'(?<!\\S)' + re.escape(best_pair[0]) + r'\\s+' + re.escape(best_pair[1]) + r'(?!\\S)'\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        new_word = re.sub(pattern, new_token, word)\n",
    "        new_word_counts[new_word] += count\n",
    "        \n",
    "    return new_word_counts\n",
    "\n",
    "def train_wordpiece(corpus, vocab_size):\n",
    "    \"\"\"\n",
    "    Trains a WordPiece tokenizer from a corpus.\n",
    "    \"\"\"\n",
    "    print(\"WordPiece: Getting word counts...\")\n",
    "    # 1. Get word frequencies\n",
    "    word_counts = get_word_counts_wp(corpus)\n",
    "    \n",
    "    # 2. Get initial vocabulary (all unique characters)\n",
    "    vocab = set()\n",
    "    for word in word_counts:\n",
    "        vocab.update(word.split())\n",
    "    \n",
    "    # Add special tokens\n",
    "    special_tokens = ['[UNK]', '[CLS]', '[SEP]', '[PAD]', '[MASK]']\n",
    "    for token in special_tokens:\n",
    "        if token not in vocab:\n",
    "            vocab.add(token)\n",
    "            \n",
    "    print(f\"WordPiece: Initial vocab size: {len(vocab)}\")\n",
    "    \n",
    "    # We iterate until we reach the target vocab size\n",
    "    num_merges = vocab_size - len(vocab)\n",
    "    if num_merges <= 0:\n",
    "        print(f\"WordPiece: Target vocab size ({vocab_size}) is smaller than initial vocab size ({len(vocab)}). No merges needed.\")\n",
    "        return vocab\n",
    "    \n",
    "    print(f\"WordPiece: Will perform {num_merges} merges.\")\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        # 3a. Get pair and individual token counts\n",
    "        pairs, token_counts = get_stats_wp(word_counts)\n",
    "        \n",
    "        if not pairs:\n",
    "            print(f\"WordPiece: No more pairs to merge. Stopped at merge {i+1}.\")\n",
    "            break\n",
    "\n",
    "        # 3b. Find the best pair based on likelihood score\n",
    "        best_pair = ('', '')\n",
    "        max_score = -1.0\n",
    "        \n",
    "        for pair, count in pairs.items():\n",
    "            token_a, token_b = pair\n",
    "            # Ensure we don't divide by zero\n",
    "            if token_counts[token_a] > 0 and token_counts[token_b] > 0:\n",
    "                # Score = freq(pair) / (freq(A) * freq(B))\n",
    "                score = count / (token_counts[token_a] * token_counts[token_b])\n",
    "            else:\n",
    "                score = 0.0\n",
    "            \n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                best_pair = pair\n",
    "        \n",
    "        # *** THIS IS THE CORRECTED LINE ***\n",
    "        if max_score == -1.0: \n",
    "            print(f\"WordPiece: No valid pairs found (max_score = -1). Stopped at merge {i+1}.\")\n",
    "            break\n",
    "\n",
    "        # 3c. Merge the pair and add to vocab\n",
    "        new_token = ''.join(best_pair)\n",
    "        vocab.add(new_token)\n",
    "        \n",
    "        # 3d. Update all word representations\n",
    "        word_counts = merge_pair_wp(best_pair, word_counts)\n",
    "        \n",
    "        if (i + 1) % 500 == 0 or i == 0:\n",
    "            print(f\"WordPiece: Merge {i+1}/{num_merges} - Vocab size: {len(vocab)} - Best pair: {best_pair} -> {new_token}\")\n",
    "            \n",
    "    print(\"WordPiece: Training complete.\")\n",
    "    return vocab\n",
    "\n",
    "def tokenize_wordpiece(text, vocab, unk_token=\"[UNK]\"):\n",
    "    \"\"\"\n",
    "    Tokenizes new text using the learned WordPiece vocabulary.\n",
    "    Uses a greedy, longest-match-first approach.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Pre-tokenize into words\n",
    "    words = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "    \n",
    "    output_tokens = []\n",
    "    \n",
    "    for word in words:\n",
    "        # If the whole word is in our vocab, great!\n",
    "        if word in vocab:\n",
    "            output_tokens.append(word)\n",
    "            continue\n",
    "            \n",
    "        word_tokens = []\n",
    "        start = 0\n",
    "        while start < len(word):\n",
    "            # Find the longest subword in vocab that matches from `start`\n",
    "            end = len(word)\n",
    "            best_subword = None\n",
    "            \n",
    "            # Greedily check from longest possible (end) to shortest (start+1)\n",
    "            while end > start:\n",
    "                sub = word[start:end]\n",
    "                # Note: WordPiece vocab doesn't store '##' prefixes\n",
    "                if sub in vocab:\n",
    "                    best_subword = sub\n",
    "                    break # Found the longest valid subword\n",
    "                end -= 1\n",
    "            \n",
    "            # Case 1: No subword found (not even a single character)\n",
    "            if best_subword is None:\n",
    "                # This means the character at word[start] is not in our vocab\n",
    "                # This is a true \"unknown\"\n",
    "                word_tokens = [unk_token] # The whole word is un-tokenizable\n",
    "                break # Stop processing this word\n",
    "            \n",
    "            # Case 2: We found a subword\n",
    "            if start == 0:\n",
    "                # It's the first piece of the word\n",
    "                word_tokens.append(best_subword)\n",
    "            else:\n",
    "                # It's a subsequent piece, add the '##' prefix\n",
    "                word_tokens.append(\"##\" + best_subword)\n",
    "            \n",
    "            # Move our `start` pointer to the end of the subword we just found\n",
    "            start += len(best_subword)\n",
    "        \n",
    "        output_tokens.extend(word_tokens)\n",
    "        \n",
    "    return output_tokens\n",
    "\n",
    "# =============================================================================\n",
    "# 3. MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- 1. Load Data ---\n",
    "    print(f\"Loading first {NUM_SAMPLES_FOR_TRAINING} lines from {LOCAL_DATA_FILE_PATH}...\")\n",
    "    corpus = []\n",
    "    try:\n",
    "        with open(LOCAL_DATA_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= NUM_SAMPLES_FOR_TRAINING:\n",
    "                    break\n",
    "                # .strip() removes leading/trailing whitespace and newlines\n",
    "                corpus.append(line.strip())\n",
    "                \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{LOCAL_DATA_FILE_PATH}' was not found.\")\n",
    "        print(\"Please update the LOCAL_DATA_FILE_PATH variable at the top of the script.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Loaded {len(corpus)} lines.\")\n",
    "    if not corpus:\n",
    "        print(\"Corpus is empty! Check your file and file path. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # --- 2. Train BPE ---\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"--- Starting BPE Training ---\")\n",
    "    print(f\"Target merges: {NUM_MERGES_BPE}\")\n",
    "    print(\"=\"*30)\n",
    "    start_time = time.time()\n",
    "    bpe_vocab, bpe_rules = train_bpe(corpus, NUM_MERGES_BPE)\n",
    "    end_time = time.time()\n",
    "    print(f\"BPE training took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    # --- 3. Train WordPiece ---\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"--- Starting WordPiece Training ---\")\n",
    "    print(f\"Target vocab size: {TARGET_VOCAB_SIZE_WP}\")\n",
    "    print(\"=\"*30)\n",
    "    start_time = time.time()\n",
    "    wp_vocab = train_wordpiece(corpus, TARGET_VOCAB_SIZE_WP)\n",
    "    end_time = time.time()\n",
    "    print(f\"WordPiece training took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "    # --- 4. Test Tokenization ---\n",
    "    test_sentence = \"ркЧрлБркЬрк░рк╛ркдркирлБркВ рк╕рлМркерлА ркорлЛркЯрлБркВ рк╢рк╣рлЗрк░ ркЕркоркжрк╛рк╡рк╛ркж ркЫрлЗ.\" # \"Ahmedabad is the largest city in Gujarat.\"\n",
    "    print(f\"\\nTest Sentence: {test_sentence}\")\n",
    "\n",
    "    # Test BPE\n",
    "    print(\"\\n--- BPE Tokenization Test ---\")\n",
    "    bpe_tokens = tokenize_bpe(test_sentence, bpe_rules)\n",
    "    print(f\"BPE Tokens: {bpe_tokens}\")\n",
    "\n",
    "    # Test WordPiece\n",
    "    print(\"\\n--- WordPiece Tokenization Test ---\")\n",
    "    wp_tokens = tokenize_wordpiece(test_sentence, wp_vocab)\n",
    "    print(f\"WordPiece Tokens: {wp_tokens}\")\n",
    "\n",
    "    # --- 5. Save Vocabularies (Optional but Recommended) ---\n",
    "    print(\"\\nSaving vocab files for 32k run...\")\n",
    "    \n",
    "    # Save BPE vocab and rules\n",
    "    with open(\"bpe_vocab_32k.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for token in sorted(list(bpe_vocab)):\n",
    "            f.write(token + \"\\n\")\n",
    "            \n",
    "    with open(\"bpe_merges_32k.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for pair in bpe_rules:\n",
    "            f.write(f\"{pair[0]} {pair[1]}\\n\")\n",
    "\n",
    "    # Save WordPiece vocab\n",
    "    with open(\"wordpiece_vocab_32k.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for token in sorted(list(wp_vocab)):\n",
    "            f.write(token + \"\\n\")\n",
    "            \n",
    "    print(\"Saved 'bpe_vocab_32k.txt', 'bpe_merges_32k.txt', and 'wordpiece_vocab_32k.txt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
