{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c8acc3",
   "metadata": {},
   "source": [
    "<h1>Implement the Katz Backoff model for the quadrigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a4861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Katz Backoff Model...\n",
      "Preprocessing follower sets for alpha calculation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2-grams: 100%|██████████| 5497241/5497241 [00:06<00:00, 867485.95it/s] \n",
      "Processing 3-grams: 100%|██████████| 9812212/9812212 [01:08<00:00, 143918.52it/s]\n",
      "Processing 4-grams: 100%|██████████| 11619213/11619213 [00:52<00:00, 219453.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete.\n",
      "\n",
      "Evaluating perplexity on validation.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|██████████| 1000/1000 [00:12<00:00, 80.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Perplexity: 4183.7198\n",
      "\n",
      "Evaluating perplexity on test.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 100%|██████████| 1000/1000 [00:03<00:00, 330.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Perplexity: 4433.3625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle, math, collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_counts(n):\n",
    "    \"\"\"Load pickled n-gram counts\"\"\"\n",
    "    with open(f\"ngram_counts_{n}gram.pkl\", \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def good_turing_discount(c, Nc):\n",
    "    \"\"\"Compute discounted count using Good–Turing\"\"\"\n",
    "    Nc_c = Nc.get(c, 0)\n",
    "    Nc_c1 = Nc.get(c + 1, 0)\n",
    "    if c == 0 or Nc_c == 0 or Nc_c1 == 0:\n",
    "        return c  # fallback to raw count\n",
    "    return (c + 1) * (Nc_c1 / Nc_c)\n",
    "\n",
    "# Katz Backoff\n",
    "class KatzBackoff:\n",
    "    def __init__(self, uni, bi, tri, quad, cutoff=5):\n",
    "        self.models = [None, uni, bi, tri, quad]\n",
    "        self.cutoff = cutoff\n",
    "        self.N = sum(uni.values())  # total tokens\n",
    "\n",
    "        # Nc tables: frequency of frequencies\n",
    "        self.Nc = [None] + [collections.Counter(m.values()) for m in self.models[1:]]\n",
    "        \n",
    "        # Followers: for alpha computation\n",
    "        self.followers = [None, {}, {}, {}]\n",
    "        for n in range(2, 5):\n",
    "            for ngram in self.models[n]:\n",
    "                context, word = ngram[:-1], ngram[-1]\n",
    "                self.followers[n-1].setdefault(context, set()).add(word)\n",
    "\n",
    "        self.alpha_cache = [None, {}, {}, {}]\n",
    "        self.prob_cache = {}\n",
    "\n",
    "    def prob_seen(self, ngram):\n",
    "        \"\"\"Discounted prob for seen n-grams\"\"\"\n",
    "        n, c = len(ngram), self.models[len(ngram)][ngram]\n",
    "        context = ngram[:-1]\n",
    "        denom = self.N if n == 1 else self.models[n-1].get(context, 0)\n",
    "        if denom == 0: return 0\n",
    "        if c > self.cutoff: return c / denom\n",
    "        return good_turing_discount(c, self.Nc[n]) / denom\n",
    " \n",
    "    def alpha(self, context):\n",
    "        \"\"\"Compute alpha(context) — backoff weight\"\"\"\n",
    "        n = len(context)\n",
    "        if context in self.alpha_cache[n]:\n",
    "            return self.alpha_cache[n][context]\n",
    "\n",
    "        seen = self.followers[n].get(context, set())\n",
    "        num = 1 - sum(self.prob(context + (w,)) for w in seen)\n",
    "        den = 1 - sum(self.prob(context[1:] + (w,)) for w in seen)\n",
    "        alpha = num / den if den > 0 else 1.0\n",
    "        self.alpha_cache[n][context] = alpha\n",
    "        return alpha\n",
    "\n",
    "    def prob(self, ngram):\n",
    "        \"\"\"Recursive Katz backoff probability\"\"\"\n",
    "        if ngram in self.prob_cache:\n",
    "            return self.prob_cache[ngram]\n",
    "\n",
    "        n = len(ngram)\n",
    "        if n == 1:\n",
    "            prob = self.prob_seen(ngram) or 1e-9\n",
    "        elif ngram in self.models[n]:\n",
    "            prob = self.prob_seen(ngram)\n",
    "        else:\n",
    "            prob = self.alpha(ngram[:-1]) * self.prob(ngram[1:])\n",
    "        \n",
    "        self.prob_cache[ngram] = prob\n",
    "        return prob\n",
    "\n",
    "    def sent_logprob(self, sentence, n=4):\n",
    "        tokens = [\"<s>\"]*(n-1) + sentence.split() + [\"</s>\"]\n",
    "        total_logp = 0\n",
    "        for i in range(n-1, len(tokens)):\n",
    "            ngram = tuple(tokens[i-n+1:i+1])\n",
    "            p = self.prob(ngram)\n",
    "            total_logp += math.log(p if p > 0 else 1e-12)\n",
    "        return total_logp, len(tokens) - (n-1)\n",
    "\n",
    "def perplexity(model, filepath):\n",
    "    total_logp, total_words = 0, 0\n",
    "    with open(filepath, encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=f\"Perplexity {filepath}\"):\n",
    "            s = line.strip()\n",
    "            if not s: continue\n",
    "            model.prob_cache.clear()\n",
    "            lp, wc = model.sent_logprob(s)\n",
    "            total_logp += lp\n",
    "            total_words += wc\n",
    "    return math.exp(-total_logp / total_words)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uni, bi, tri, quad = [load_counts(i) for i in range(1,5)]\n",
    "    model = KatzBackoff(uni, bi, tri, quad)\n",
    "\n",
    "    val_ppl = perplexity(model, \"validation.txt\")\n",
    "    print(f\"Validation Perplexity: {val_ppl:.3f}\")\n",
    "\n",
    "    test_ppl = perplexity(model, \"test.txt\")\n",
    "    print(f\"Test Perplexity: {test_ppl:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae23fab3",
   "metadata": {},
   "source": [
    "<h1>Implement the Kneser-Ney smoothing algorithm for the quadrigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc4e709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all n-gram count files...\n",
      "Successfully loaded ngram_counts_1gram.pkl\n",
      "Successfully loaded ngram_counts_2gram.pkl\n",
      "Successfully loaded ngram_counts_3gram.pkl\n",
      "Successfully loaded ngram_counts_4gram.pkl\n",
      "Performing pre-calculations for Kneser-Ney...\n",
      "\n",
      "Evaluating perplexity on validation.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 1000it [00:00, 1455.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Perplexity: 1375.6543\n",
      "\n",
      "Evaluating perplexity on test.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Perplexity: 1000it [00:00, 2841.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Perplexity: 1552.9169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import collections\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Loading all n-gram count files...\")\n",
    "all_counts = [None] \n",
    "for n in range(1, 5):\n",
    "    file_path = f\"ngram_counts_{n}gram.pkl\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            counts = pickle.load(f)\n",
    "            all_counts.append(counts)\n",
    "        print(f\"Successfully loaded {file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find the file {file_path}. Please ensure it's in the correct directory.\")\n",
    "        exit() \n",
    "\n",
    "unigrams, bigrams, trigrams, quadrigrams = all_counts[1:]\n",
    "DISCOUNT = 0.75\n",
    "\n",
    "print(\"Performing pre-calculations for Kneser-Ney...\")\n",
    "preceders_of_word = collections.defaultdict(set)\n",
    "for w1, w2 in bigrams:\n",
    "    preceders_of_word[w2].add(w1)\n",
    "total_bigram_types = len(bigrams)\n",
    "\n",
    "followers_of_unigram = collections.defaultdict(set)\n",
    "for w1, w2 in bigrams:\n",
    "    followers_of_unigram[(w1,)].add(w2)\n",
    "\n",
    "followers_of_bigram = collections.defaultdict(set)\n",
    "for w1, w2, w3 in trigrams:\n",
    "    followers_of_bigram[(w1, w2)].add(w3)\n",
    "\n",
    "followers_of_trigram = collections.defaultdict(set)\n",
    "for w1, w2, w3, w4 in quadrigrams:\n",
    "    followers_of_trigram[(w1, w2, w3)].add(w4)\n",
    "\n",
    "def get_kn_prob(ngram):\n",
    "    if len(ngram) == 1:\n",
    "        word = ngram[0]\n",
    "        numerator = len(preceders_of_word.get(word, set()))\n",
    "        return numerator / total_bigram_types if total_bigram_types > 0 else 0\n",
    "\n",
    "    n = len(ngram)\n",
    "    context = ngram[:-1]\n",
    "    count_ngram = all_counts[n].get(ngram, 0)\n",
    "    count_context = all_counts[n-1].get(context, 0)\n",
    "\n",
    "    if count_context == 0:\n",
    "        return get_kn_prob(ngram[1:])\n",
    "\n",
    "    first_term = max(count_ngram - DISCOUNT, 0) / count_context\n",
    "\n",
    "    if n == 4: followers = followers_of_trigram.get(context, set())\n",
    "    elif n == 3: followers = followers_of_bigram.get(context, set())\n",
    "    else: followers = followers_of_unigram.get(context, set())\n",
    "    \n",
    "    lambda_weight = (DISCOUNT / count_context) * len(followers)\n",
    "\n",
    "    lower_order_prob = get_kn_prob(ngram[1:])\n",
    "\n",
    "    return first_term + lambda_weight * lower_order_prob\n",
    "\n",
    "def calculate_perplexity(filepath):\n",
    "    print(f\"\\nEvaluating perplexity on {filepath}...\")\n",
    "    total_log_prob = 0\n",
    "    total_words = 0\n",
    "    n = 4\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for sentence in tqdm(f, desc=\"Calculating Perplexity\"):\n",
    "            if not sentence.strip(): continue\n",
    "            tokens = [\"<s>\"] * (n - 1) + sentence.strip().split() + [\"</s>\"]\n",
    "            \n",
    "            for i in range(n - 1, len(tokens)):\n",
    "                ngram = tuple(tokens[i - n + 1 : i + 1])\n",
    "                prob = get_kn_prob(ngram)\n",
    "                total_log_prob += math.log(prob if prob > 0 else 1e-12)\n",
    "            \n",
    "            total_words += len(sentence.strip().split()) + 1\n",
    "\n",
    "    if total_words == 0: return float('inf')\n",
    "    \n",
    "    perplexity = math.exp(-total_log_prob / total_words)\n",
    "    return perplexity\n",
    "\n",
    "val_perplexity = calculate_perplexity(\"validation.txt\")\n",
    "print(f\"Validation Set Perplexity: {val_perplexity:.4f}\")\n",
    "\n",
    "test_perplexity = calculate_perplexity(\"test.txt\")\n",
    "print(f\"Test Set Perplexity: {test_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ed37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all n-gram count files...\n",
      "Successfully loaded ngram_counts_1gram.pkl\n",
      "Successfully loaded ngram_counts_2gram.pkl\n",
      "Successfully loaded ngram_counts_3gram.pkl\n",
      "Successfully loaded ngram_counts_4gram.pkl\n",
      "Performing pre-calculations for Kneser-Ney...\n",
      "\n",
      "Calculating probabilities for validation.txt...\n",
      "Results will be saved to validation_probabilities.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 1000it [00:00, 1597.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating probabilities for test.txt...\n",
      "Results will be saved to test_probabilities.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 1000it [00:00, 3972.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import collections\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import csv \n",
    "print(\"Loading all n-gram count files...\")\n",
    "all_counts = [None]\n",
    "for n in range(1, 5):\n",
    "    file_path = f\"ngram_counts_{n}gram.pkl\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            counts = pickle.load(f)\n",
    "            all_counts.append(counts)\n",
    "        print(f\"Successfully loaded {file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find the file {file_path}. Please ensure it's in the correct directory.\")\n",
    "        exit()\n",
    "\n",
    "unigrams, bigrams, trigrams, quadrigrams = all_counts[1:]\n",
    "DISCOUNT = 0.75\n",
    "\n",
    "print(\"Performing pre-calculations for Kneser-Ney...\")\n",
    "preceders_of_word = collections.defaultdict(set)\n",
    "for w1, w2 in bigrams:\n",
    "    preceders_of_word[w2].add(w1)\n",
    "total_bigram_types = len(bigrams)\n",
    "\n",
    "followers_of_unigram = collections.defaultdict(set)\n",
    "for w1, w2 in bigrams:\n",
    "    followers_of_unigram[(w1,)].add(w2)\n",
    "\n",
    "followers_of_bigram = collections.defaultdict(set)\n",
    "for w1, w2, w3 in trigrams:\n",
    "    followers_of_bigram[(w1, w2)].add(w3)\n",
    "\n",
    "followers_of_trigram = collections.defaultdict(set)\n",
    "for w1, w2, w3, w4 in quadrigrams:\n",
    "    followers_of_trigram[(w1, w2, w3)].add(w4)\n",
    "\n",
    "def get_kn_prob(ngram):\n",
    "    \"\"\"Calculates the Kneser-Ney probability of an n-gram recursively.\"\"\"\n",
    "    if len(ngram) == 1:\n",
    "        word = ngram[0]\n",
    "        numerator = len(preceders_of_word.get(word, set()))\n",
    "        return numerator / total_bigram_types if total_bigram_types > 0 else 0\n",
    "\n",
    "    n = len(ngram)\n",
    "    context = ngram[:-1]\n",
    "    count_ngram = all_counts[n].get(ngram, 0)\n",
    "    count_context = all_counts[n-1].get(context, 0)\n",
    "\n",
    "    if count_context == 0:\n",
    "        return get_kn_prob(ngram[1:])\n",
    "\n",
    "    first_term = max(count_ngram - DISCOUNT, 0) / count_context\n",
    "\n",
    "    if n == 4: followers = followers_of_trigram.get(context, set())\n",
    "    elif n == 3: followers = followers_of_bigram.get(context, set())\n",
    "    else: followers = followers_of_unigram.get(context, set())\n",
    "    lambda_weight = (DISCOUNT / count_context) * len(followers)\n",
    "\n",
    "    lower_order_prob = get_kn_prob(ngram[1:])\n",
    "\n",
    "    return first_term + lambda_weight * lower_order_prob\n",
    "\n",
    "# --- MODIFIED EVALUATION FUNCTION ---\n",
    "def evaluate_sentence_probabilities(filepath):\n",
    "    \"\"\"Calculates and saves the probability and log-prob for each sentence.\"\"\"\n",
    "    output_filename = filepath.replace('.txt', '_probabilities.csv')\n",
    "    print(f\"\\nCalculating probabilities for {filepath}...\")\n",
    "    print(f\"Results will be saved to {output_filename}\")\n",
    "\n",
    "    n = 4 # Quadrigram model\n",
    "\n",
    "    # Open the output CSV file to write the results\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "         open(output_filename, \"w\", encoding=\"utf-8\", newline='') as outfile:\n",
    "\n",
    "        writer = csv.writer(outfile)\n",
    "        # Write the header row for the CSV file\n",
    "        writer.writerow([\"Sentence\", \"Probability\", \"Log Probability\"])\n",
    "\n",
    "        for sentence in tqdm(infile, desc=\"Processing Sentences\"):\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence: continue\n",
    "\n",
    "            # Initialize the log probability for the current sentence\n",
    "            sentence_log_prob = 0.0\n",
    "            \n",
    "            # Pad the sentence with start and end tokens\n",
    "            tokens = [\"<s>\"] * (n - 1) + sentence.split() + [\"</s>\"]\n",
    "\n",
    "            # Calculate the log probability by summing the log probs of its n-grams\n",
    "            for i in range(n - 1, len(tokens)):\n",
    "                ngram = tuple(tokens[i - n + 1 : i + 1])\n",
    "                prob = get_kn_prob(ngram)\n",
    "                sentence_log_prob += math.log(prob if prob > 0 else 1e-12)\n",
    "            \n",
    "            # Convert the total log probability back to a regular probability\n",
    "            # P(sentence) = exp(log(P(sentence)))\n",
    "            sentence_prob = math.exp(sentence_log_prob)\n",
    "            \n",
    "            # Write the sentence and its calculated scores to the CSV file\n",
    "            writer.writerow([sentence, f\"{sentence_prob:.4e}\", f\"{sentence_log_prob:.4f}\"])\n",
    "\n",
    "# --- MODIFIED EXECUTION BLOCK ---\n",
    "evaluate_sentence_probabilities(\"validation.txt\")\n",
    "evaluate_sentence_probabilities(\"test.txt\")\n",
    "\n",
    "print(\"\\nProcessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc03960",
   "metadata": {},
   "source": [
    "<h1>For each of the n-gram models generate 100 sentences using<br>\n",
    "a. Greedy Approach (using maximum likelihood estimation)<br>\n",
    "b. Beam Search with beam size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50d7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Sentence Generator...\n",
      "Initialization complete.\n",
      "\n",
      "\n",
      "Generating 20 Greedy 2-gram sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Greedy 2-gram: 100%|██████████| 20/20 [00:00<00:00, 91678.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 20 Top-p 2-gram sentences (p=0.9)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Top-p 2-gram: 100%|██████████| 20/20 [00:06<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 20 Beam Search 2-gram sentences (k=10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam 2-gram: 100%|██████████| 20/20 [00:20<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 2-gram results: greedy_2gram.txt | topp_2gram.txt | beam_2gram.txt\n",
      "\n",
      "Generating 20 Greedy 3-gram sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Greedy 3-gram: 100%|██████████| 20/20 [00:00<00:00, 91678.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 20 Top-p 3-gram sentences (p=0.9)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Top-p 3-gram: 100%|██████████| 20/20 [00:05<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 20 Beam Search 3-gram sentences (k=10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam 3-gram: 100%|██████████| 20/20 [00:20<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 3-gram results: greedy_3gram.txt | topp_3gram.txt | beam_3gram.txt\n",
      "\n",
      "Generating 20 Greedy 4-gram sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Greedy 4-gram: 100%|██████████| 20/20 [00:00<00:00, 162569.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 20 Top-p 4-gram sentences (p=0.9)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Top-p 4-gram: 100%|██████████| 20/20 [00:05<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 20 Beam Search 4-gram sentences (k=10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam 4-gram: 100%|██████████| 20/20 [00:20<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 4-gram results: greedy_4gram.txt | topp_4gram.txt | beam_4gram.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import math\n",
    "import heapq\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_ngram_counts(n):\n",
    "    file_name = f\"ngram_counts_{n}gram.pkl\"\n",
    "    try:\n",
    "        with open(file_name, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_name} was not found.\")\n",
    "        exit()\n",
    "\n",
    "class SentenceGenerator:\n",
    "    def __init__(self, counts1, counts2, counts3, counts4):\n",
    "        print(\"Initializing Sentence Generator...\")\n",
    "        self.unigrams = counts1\n",
    "        self.bigram_map = self._preprocess_and_sort_counts(counts2)\n",
    "        self.trigram_map = self._preprocess_and_sort_counts(counts3)\n",
    "        self.quadrigram_map = self._preprocess_and_sort_counts(counts4)\n",
    "        self.all_counts = [None, counts1, counts2, counts3, counts4]\n",
    "        self.all_maps = [None, None, self.bigram_map, self.trigram_map, self.quadrigram_map]\n",
    "\n",
    "        total = sum(self.unigrams.values())\n",
    "        self.unigram_probs = [(w[0], c / total) for w, c in self.unigrams.items()]\n",
    "        self.sorted_unigrams = sorted(self.unigram_probs, key=lambda x: x[1], reverse=True)\n",
    "        print(\"Initialization complete.\\n\")\n",
    "\n",
    "    def _preprocess_and_sort_counts(self, ngram_counts):\n",
    "        \"\"\"Creates sorted map: context → [(word, count)...]\"\"\"\n",
    "        processed = {}\n",
    "        for ngram, count in ngram_counts.items():\n",
    "            ctx, w = ngram[:-1], ngram[-1]\n",
    "            processed.setdefault(ctx, {})[w] = count\n",
    "        sorted_map = {ctx: sorted(words.items(), key=lambda x: x[1], reverse=True)\n",
    "                      for ctx, words in processed.items()}\n",
    "        return sorted_map\n",
    "\n",
    "    def _get_probs(self, context, top_k=None):\n",
    "        \"\"\"Backoff to lower n if context unseen.\"\"\"\n",
    "        n = len(context) + 1\n",
    "        if n == 1:\n",
    "            return self.sorted_unigrams\n",
    "\n",
    "        next_words = self.all_maps[n].get(context)\n",
    "        if not next_words:\n",
    "            return self._get_probs(context[1:], top_k)\n",
    "\n",
    "        if top_k:\n",
    "            next_words = next_words[:top_k]\n",
    "        ctx_count = self.all_counts[n - 1].get(context, 0)\n",
    "        if ctx_count == 0:\n",
    "            return self._get_probs(context[1:], top_k)\n",
    "        return [(w, c / ctx_count) for w, c in next_words]\n",
    "\n",
    "\n",
    "    # ---------- Generation methods ----------\n",
    "    def generate_greedy(self, n, num_sentences=100, max_len=30):\n",
    "        print(f\"\\nGenerating {num_sentences} Greedy {n}-gram sentences...\")\n",
    "        sentences = []\n",
    "        for _ in tqdm(range(num_sentences), desc=f\"Greedy {n}-gram\"):\n",
    "            context = tuple([\"<s>\"] * (n - 1))\n",
    "            sentence = []\n",
    "            for _ in range(max_len):\n",
    "                probs = self._get_probs(context)\n",
    "                if not probs: break\n",
    "                word = probs[0][0]  # most probable\n",
    "                if word == \"</s>\": break\n",
    "                sentence.append(word)\n",
    "                if n > 1: context = context[1:] + (word,)\n",
    "            sentences.append(\" \".join(sentence))\n",
    "        return sentences\n",
    "\n",
    "    def generate_beam(self, n, num_sentences=100, beam_width=10, max_len=30):\n",
    "        print(f\"\\nGenerating {num_sentences} Beam Search {n}-gram sentences (k={beam_width})...\")\n",
    "        sentences = []\n",
    "        for _ in tqdm(range(num_sentences), desc=f\"Beam {n}-gram\"):\n",
    "            beam = [(0.0, [\"<s>\"] * (n - 1))]\n",
    "            for _ in range(max_len):\n",
    "                all_candidates = []\n",
    "                for log_p, seq in beam:\n",
    "                    if seq[-1] == \"</s>\":\n",
    "                        all_candidates.append((log_p, seq))\n",
    "                        continue\n",
    "                    context = tuple(seq[-(n - 1):])\n",
    "                    next_probs = self._get_probs(context, top_k=beam_width * 2)\n",
    "                    if not next_probs:\n",
    "                        continue\n",
    "                    for w, p in next_probs:\n",
    "                        p = max(p, 1e-12)\n",
    "                        all_candidates.append((log_p + math.log(p) + random.uniform(-1e-4, 1e-4), seq + [w]))\n",
    "                beam = heapq.nlargest(beam_width, all_candidates, key=lambda x: x[0])\n",
    "                if all(b[1][-1] == \"</s>\" for b in beam):\n",
    "                    break\n",
    "            best_seq = max(beam, key=lambda x: x[0])[1]\n",
    "            sent = \" \".join([w for w in best_seq if w not in (\"<s>\", \"</s>\")])\n",
    "            sentences.append(sent)\n",
    "        return sentences\n",
    "\n",
    "# ----------------- Main Execution -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    unigrams = load_ngram_counts(1)\n",
    "    bigrams = load_ngram_counts(2)\n",
    "    trigrams = load_ngram_counts(3)\n",
    "    quadrigrams = load_ngram_counts(4)\n",
    "\n",
    "    gen = SentenceGenerator(unigrams, bigrams, trigrams, quadrigrams)\n",
    "\n",
    "    for n in range(2, 5):\n",
    "        greedy = gen.generate_greedy(n=n, num_sentences=20)\n",
    "        with open(f\"greedy_{n}gram.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines([s + \"\\n\" for s in greedy])\n",
    "\n",
    "        top_p = gen.generate_top_p(n=n, num_sentences=20, p_threshold=0.9)\n",
    "        with open(f\"topp_{n}gram.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines([s + \"\\n\" for s in top_p])\n",
    "\n",
    "        beam = gen.generate_beam(n=n, num_sentences=20, beam_width=10)\n",
    "        with open(f\"beam_{n}gram.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines([s + \"\\n\" for s in beam])\n",
    "\n",
    "        print(f\"✅ Saved {n}-gram results: greedy_{n}gram.txt | topp_{n}gram.txt | beam_{n}gram.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
