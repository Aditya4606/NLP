{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac478f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100000 lines...\n",
      "Processed 200000 lines...\n",
      "Processed 300000 lines...\n",
      "Processed 400000 lines...\n",
      "Processed 500000 lines...\n",
      "Processed 600000 lines...\n",
      "Processed 700000 lines...\n",
      "Processed 800000 lines...\n",
      "Processed 900000 lines...\n",
      "Processed 1000000 lines...\n",
      "Saved 1-gram counts to ngram_counts_1gram.pkl\n",
      "Saved 2-gram counts to ngram_counts_2gram.pkl\n",
      "Saved 3-gram counts to ngram_counts_3gram.pkl\n",
      "Saved 4-gram counts to ngram_counts_4gram.pkl\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "def count_ngrams(file_path, num_lines=None, n_list=[1,2,3,4]):\n",
    "    \"\"\"\n",
    "    Count n-grams (1-gram, 2-gram, 3-gram, 4-gram) from a large tokenized file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the corpus (one tokenized sentence per line)\n",
    "        num_lines: Number of lines to read (None = read all)\n",
    "        n_list: List of n-gram sizes to count (e.g., [1,2,3,4])\n",
    "    \n",
    "    Returns:\n",
    "        dict: {n: defaultdict(counts)}\n",
    "    \"\"\"\n",
    "    ngram_counts = {n: defaultdict(int) for n in n_list}\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if num_lines and i >= num_lines:   ##if num of lines> 1M THEN EXIT  \n",
    "                break\n",
    "\n",
    "            tokens = line.strip().split()\n",
    "            for n in n_list:\n",
    "                # Padding only for n > 1\n",
    "                if n > 1:\n",
    "                    tokens_n = [\"<s>\"]*(n-1) + tokens + [\"</s>\"]\n",
    "                else:\n",
    "                    tokens_n = tokens\n",
    "                for j in range(len(tokens_n) - n + 1):\n",
    "                    ngram = tuple(tokens_n[j:j+n])\n",
    "                    ngram_counts[n][ngram] += 1\n",
    "\n",
    "            if (i+1) % 100000 == 0:\n",
    "                print(f\"Processed {i+1} lines...\")\n",
    "\n",
    "    return ngram_counts\n",
    "\n",
    "\n",
    "def save_counts_to_file(ngram_counts, prefix=\"ngram_counts\"):\n",
    "    \"\"\"\n",
    "    Save n-gram counts to disk using pickle.\n",
    "    \"\"\"\n",
    "    for n, counts in ngram_counts.items():\n",
    "        file_name = f\"{prefix}_{n}gram.pkl\"\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            pickle.dump(dict(counts), f)\n",
    "        print(f\"Saved {n}-gram counts to {file_name}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    corpus_file = \"tokenized_sentences.txt\"\n",
    "    num_lines_to_process = 1_000_000  \n",
    "\n",
    "    # Count 1-gram to 4-gram\n",
    "    counts = count_ngrams(corpus_file, num_lines=num_lines_to_process, n_list=[1,2,3,4])\n",
    "\n",
    "    # Save to disk\n",
    "    save_counts_to_file(counts, prefix=\"ngram_counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6f06c1",
   "metadata": {},
   "source": [
    "<h1>printing 10 lines of all the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2236f152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 items from ngram_counts_1gram.pkl:\n",
      "('આ',) 161255\n",
      "('વીડિયો',) 4200\n",
      "('જુઓ:',) 194\n",
      "('ઊંઝા',) 112\n",
      "('માર્કેટયાર્ડ',) 27\n",
      "('આજથી',) 977\n",
      "('25',) 1939\n",
      "('જુલાઈ',) 683\n",
      "('સુધી',) 23874\n",
      "('બંધ',) 8839\n",
      "\n",
      "\n",
      "First 10 items from ngram_counts_2gram.pkl:\n",
      "('<s>', 'આ') 67993\n",
      "('આ', 'વીડિયો') 873\n",
      "('વીડિયો', 'જુઓ:') 36\n",
      "('જુઓ:', 'ઊંઝા') 1\n",
      "('ઊંઝા', 'માર્કેટયાર્ડ') 1\n",
      "('માર્કેટયાર્ડ', 'આજથી') 1\n",
      "('આજથી', '25') 2\n",
      "('25', 'જુલાઈ') 5\n",
      "('જુલાઈ', 'સુધી') 113\n",
      "('સુધી', 'બંધ') 136\n",
      "\n",
      "\n",
      "First 10 items from ngram_counts_3gram.pkl:\n",
      "('<s>', '<s>', 'આ') 67993\n",
      "('<s>', 'આ', 'વીડિયો') 487\n",
      "('આ', 'વીડિયો', 'જુઓ:') 36\n",
      "('વીડિયો', 'જુઓ:', 'ઊંઝા') 1\n",
      "('જુઓ:', 'ઊંઝા', 'માર્કેટયાર્ડ') 1\n",
      "('ઊંઝા', 'માર્કેટયાર્ડ', 'આજથી') 1\n",
      "('માર્કેટયાર્ડ', 'આજથી', '25') 1\n",
      "('આજથી', '25', 'જુલાઈ') 1\n",
      "('25', 'જુલાઈ', 'સુધી') 1\n",
      "('જુલાઈ', 'સુધી', 'બંધ') 5\n",
      "\n",
      "\n",
      "First 10 items from ngram_counts_4gram.pkl:\n",
      "('<s>', '<s>', '<s>', 'આ') 67993\n",
      "('<s>', '<s>', 'આ', 'વીડિયો') 487\n",
      "('<s>', 'આ', 'વીડિયો', 'જુઓ:') 36\n",
      "('આ', 'વીડિયો', 'જુઓ:', 'ઊંઝા') 1\n",
      "('વીડિયો', 'જુઓ:', 'ઊંઝા', 'માર્કેટયાર્ડ') 1\n",
      "('જુઓ:', 'ઊંઝા', 'માર્કેટયાર્ડ', 'આજથી') 1\n",
      "('ઊંઝા', 'માર્કેટયાર્ડ', 'આજથી', '25') 1\n",
      "('માર્કેટયાર્ડ', 'આજથી', '25', 'જુલાઈ') 1\n",
      "('આજથી', '25', 'જુલાઈ', 'સુધી') 1\n",
      "('25', 'જુલાઈ', 'સુધી', 'બંધ') 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def read_ngram_pickle(file_path, num_lines=10):\n",
    "    \"\"\"\n",
    "    Read n-gram pickle file and print first `num_lines` items.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        ngram_counts = pickle.load(f)\n",
    "\n",
    "    print(f\"First {num_lines} items from {file_path}:\")\n",
    "    for i, (ngram, count) in enumerate(ngram_counts.items()):\n",
    "        print(ngram, count)\n",
    "        if i+1 >= num_lines:\n",
    "            break\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for n in range(1, 5):\n",
    "        file_name = f\"ngram_counts_{n}gram.pkl\"\n",
    "        read_ngram_pickle(file_name, num_lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c8889b",
   "metadata": {},
   "source": [
    "<h1>applying smoothing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2938567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All smoothed n-grams saved successfully in a memory-efficient way.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# ---------------- Load Pickle ----------------\n",
    "def load_ngram_counts(n):\n",
    "    file_name = f\"ngram_counts_{n}gram.pkl\"\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        counts = pickle.load(f)\n",
    "    return counts\n",
    "\n",
    "# ---------------- Vocabulary ----------------\n",
    "def get_vocabulary(unigrams):                          ##extract unique words from unigram keys\n",
    "    return set([token[0] for token in unigrams.keys()])\n",
    "\n",
    "# ---------------- Generator for Add-K / Add-One Smoothing ----------------\n",
    "def smoothing_generator(ngrams, lower_ngrams=None, vocab=None, k=1.0):\n",
    "    \"\"\"\n",
    "    Generator to yield (ngram, smoothed_prob) one by one.\n",
    "    k=1.0 => Add-One Smoothing\n",
    "    k=<other> => Add-K Smoothing\n",
    "    \"\"\"\n",
    "    V = len(vocab)\n",
    "    \n",
    "    if lower_ngrams is None:  # Unigrams\n",
    "        N = sum(ngrams.values())\n",
    "        for ngram, count in ngrams.items():\n",
    "            yield ngram, (count + k) / (N + k * V)\n",
    "    else:  # n-grams n>1\n",
    "        for ngram, count in ngrams.items():\n",
    "            prefix = ngram[:-1]\n",
    "            prefix_count = lower_ngrams.get(prefix, 0)\n",
    "            yield ngram, (count + k) / (prefix_count + k * V)\n",
    "\n",
    "# ---------------- Generator for Token Type Smoothing ----------------\n",
    "def token_type_smoothing_generator(ngrams):\n",
    "    \"\"\"\n",
    "    Yield (ngram, count + number_of_token_types) one by one\n",
    "    \"\"\"\n",
    "    token_types = len(set([w for ngram in ngrams.keys() for w in ngram]))\n",
    "    for ngram, count in ngrams.items():\n",
    "        yield ngram, count + token_types\n",
    "\n",
    "# ---------------- Save generator to file in chunks ----------------\n",
    "def save_generator_to_file(generator, out_file, batch_size=1_000_000):\n",
    "    batch = {}\n",
    "    for i, (ngram, value) in enumerate(generator, 1):\n",
    "        batch[ngram] = value\n",
    "        if i % batch_size == 0:\n",
    "            with open(out_file, \"ab\") as f:\n",
    "                pickle.dump(batch, f)\n",
    "            batch = {}\n",
    "    # Save remaining\n",
    "    if batch:\n",
    "        with open(out_file, \"ab\") as f:\n",
    "            pickle.dump(batch, f)\n",
    "\n",
    "# ---------------- Main ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load all n-grams\n",
    "    unigrams = load_ngram_counts(1)\n",
    "    bigrams = load_ngram_counts(2)\n",
    "    trigrams = load_ngram_counts(3)\n",
    "    quadrigrams = load_ngram_counts(4)\n",
    "\n",
    "    vocab = get_vocabulary(unigrams)\n",
    "\n",
    "    # --- Add-One Smoothing (k=1.0) ---\n",
    "    save_generator_to_file(smoothing_generator(unigrams, vocab=vocab, k=1.0), \"unigram_addone.pkl\")\n",
    "    save_generator_to_file(smoothing_generator(bigrams, lower_ngrams=unigrams, vocab=vocab, k=1.0), \"bigram_addone.pkl\")\n",
    "    save_generator_to_file(smoothing_generator(trigrams, lower_ngrams=bigrams, vocab=vocab, k=1.0), \"trigram_addone.pkl\")\n",
    "    save_generator_to_file(smoothing_generator(quadrigrams, lower_ngrams=trigrams, vocab=vocab, k=1.0), \"quadrigram_addone.pkl\")\n",
    "\n",
    "    # --- Add-K Smoothing (k=0.5) ---\n",
    "    save_generator_to_file(smoothing_generator(unigrams, vocab=vocab, k=0.5), \"unigram_addk.pkl\")\n",
    "    save_generator_to_file(smoothing_generator(bigrams, lower_ngrams=unigrams, vocab=vocab, k=0.5), \"bigram_addk.pkl\")\n",
    "    save_generator_to_file(smoothing_generator(trigrams, lower_ngrams=bigrams, vocab=vocab, k=0.5), \"trigram_addk.pkl\")\n",
    "    save_generator_to_file(smoothing_generator(quadrigrams, lower_ngrams=trigrams, vocab=vocab, k=0.5), \"quadrigram_addk.pkl\")\n",
    "\n",
    "    # --- Token Type Smoothing ---\n",
    "    save_generator_to_file(token_type_smoothing_generator(unigrams), \"unigram_toktype.pkl\")\n",
    "    save_generator_to_file(token_type_smoothing_generator(bigrams), \"bigram_toktype.pkl\")\n",
    "    save_generator_to_file(token_type_smoothing_generator(trigrams), \"trigram_toktype.pkl\")\n",
    "    save_generator_to_file(token_type_smoothing_generator(quadrigrams), \"quadrigram_toktype.pkl\")\n",
    "\n",
    "    print(\"All smoothed n-grams saved successfully in a memory-efficient way.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e3fdac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading file: unigram_addone.pkl\n",
      "('આ',) 0.011662559910782024\n",
      "('વીડિયો',) 0.0003038300229771003\n",
      "('જુઓ:',) 1.4103036058208653e-05\n",
      "('ઊંઝા',) 8.172528587577323e-06\n",
      "('માર્કેટયાર્ડ',) 2.0250513314350887e-06\n",
      "('આજથી',) 7.073215007655417e-05\n",
      "('25',) 0.00014030712796371688\n",
      "('જુલાઈ',) 4.946911109648574e-05\n",
      "('સુધી',) 0.001726717876357598\n",
      "('બંધ',) 0.0006393376346387923\n",
      "\n",
      "Reading file: bigram_addone.pkl\n",
      "('<s>', 'આ') 0.09848408038168846\n",
      "('આ', 'વીડિયો') 0.0010262299201207992\n",
      "('વીડિયો', 'જુઓ:') 5.326760782371589e-05\n",
      "('જુઓ:', 'ઊંઝા') 2.8960324355632785e-06\n",
      "('ઊંઝા', 'માર્કેટયાર્ડ') 2.8963763435565762e-06\n",
      "('માર્કેટયાર્ડ', 'આજથી') 2.89673291977643e-06\n",
      "('આજથી', '25') 4.339128963251917e-06\n",
      "('25', 'જુલાઈ') 8.666199654796381e-06\n",
      "('જુલાઈ', 'સુધી') 0.00016495704605340268\n",
      "('સુધી', 'બંધ') 0.0001918015344122753\n",
      "\n",
      "Reading file: trigram_addone.pkl\n",
      "('<s>', '<s>', 'આ') 0.09848408038168846\n",
      "('<s>', 'આ', 'વીડિયો') 0.0006434607640569146\n",
      "('આ', 'વીડિયો', 'જુઓ:') 5.3523975124370914e-05\n",
      "('વીડિયો', 'જુઓ:', 'ઊંઝા') 2.8966951604913955e-06\n",
      "('જુઓ:', 'ઊંઝા', 'માર્કેટયાર્ડ') 2.896842007685322e-06\n",
      "('ઊંઝા', 'માર્કેટયાર્ડ', 'આજથી') 2.896842007685322e-06\n",
      "('માર્કેટયાર્ડ', 'આજથી', '25') 2.896842007685322e-06\n",
      "('આજથી', '25', 'જુલાઈ') 2.8968378118445905e-06\n",
      "('25', 'જુલાઈ', 'સુધી') 2.896825224395324e-06\n",
      "('જુલાઈ', 'સુધી', 'બંધ') 8.689116447194067e-06\n",
      "\n",
      "Reading file: quadrigram_addone.pkl\n",
      "('<s>', '<s>', '<s>', 'આ') 0.09848408038168846\n",
      "('<s>', '<s>', 'આ', 'વીડિયો') 0.0006434607640569146\n",
      "('<s>', 'આ', 'વીડિયો', 'જુઓ:') 5.355387882059885e-05\n",
      "('આ', 'વીડિયો', 'જુઓ:', 'ઊંઝા') 2.8966951604913955e-06\n",
      "('વીડિયો', 'જુઓ:', 'ઊંઝા', 'માર્કેટયાર્ડ') 2.896842007685322e-06\n",
      "('જુઓ:', 'ઊંઝા', 'માર્કેટયાર્ડ', 'આજથી') 2.896842007685322e-06\n",
      "('ઊંઝા', 'માર્કેટયાર્ડ', 'આજથી', '25') 2.896842007685322e-06\n",
      "('માર્કેટયાર્ડ', 'આજથી', '25', 'જુલાઈ') 2.896842007685322e-06\n",
      "('આજથી', '25', 'જુલાઈ', 'સુધી') 2.896842007685322e-06\n",
      "('25', 'જુલાઈ', 'સુધી', 'બંધ') 2.896842007685322e-06\n",
      "\n",
      "Reading file: unigram_addk.pkl\n",
      "('આ',) 0.011961148251836744\n",
      "('વીડિયો',) 0.00031157264857223626\n",
      "('જુઓ:',) 1.4427063479895237e-05\n",
      "('ઊંઝા',) 8.344702526931692e-06\n",
      "('માર્કેટયાર્ડ',) 2.0398161732499692e-06\n",
      "('આજથી',) 7.250619306733983e-05\n",
      "('25',) 0.00014386267156430238\n",
      "('જુલાઈ',) 5.069870379695833e-05\n",
      "('સુધી',) 0.0017708942264820506\n",
      "('બંધ',) 0.0006556710932161129\n",
      "\n",
      "Reading file: bigram_addk.pkl\n",
      "('<s>', 'આ') 0.19696671234027513\n",
      "('આ', 'વીડિયો') 0.001724723471640294\n",
      "('વીડિયો', 'જુઓ:') 0.00010446389985203332\n",
      "('જુઓ:', 'ઊંઝા') 4.342828686989175e-06\n",
      "('ઊંઝા', 'માર્કેટયાર્ડ') 4.343859953955085e-06\n",
      "('માર્કેટયાર્ડ', 'આજથી') 4.344929467311647e-06\n",
      "('આજથી', '25') 7.221676584435843e-06\n",
      "('25', 'જુલાઈ') 1.5843660519326386e-05\n",
      "('જુલાઈ', 'સુધી') 0.00032814279849430157\n",
      "('સુધી', 'બંધ') 0.00036984152358450945\n",
      "\n",
      "Reading file: trigram_addk.pkl\n",
      "('<s>', '<s>', 'આ') 0.19696671234027513\n",
      "('<s>', 'આ', 'વીડિયો') 0.0011798274910696134\n",
      "('આ', 'વીડિયો', 'જુઓ:') 0.00010546816306244872\n",
      "('વીડિયો', 'જુઓ:', 'ઊંઝા') 4.344816199792028e-06\n",
      "('જુઓ:', 'ઊંઝા', 'માર્કેટયાર્ડ') 4.345256717766886e-06\n",
      "('ઊંઝા', 'માર્કેટયાર્ડ', 'આજથી') 4.345256717766886e-06\n",
      "('માર્કેટયાર્ડ', 'આજથી', '25') 4.345256717766886e-06\n",
      "('આજથી', '25', 'જુલાઈ') 4.345244130299387e-06\n",
      "('25', 'જુલાઈ', 'સુધી') 4.345206368334453e-06\n",
      "('જુલાઈ', 'સુધી', 'બંધ') 1.592744037345504e-05\n",
      "\n",
      "Reading file: quadrigram_addk.pkl\n",
      "('<s>', '<s>', '<s>', 'આ') 0.19696671234027513\n",
      "('<s>', '<s>', 'આ', 'વીડિયો') 0.0011798274910696134\n",
      "('<s>', 'આ', 'વીડિયો', 'જુઓ:') 0.00010558592959009517\n",
      "('આ', 'વીડિયો', 'જુઓ:', 'ઊંઝા') 4.344816199792028e-06\n",
      "('વીડિયો', 'જુઓ:', 'ઊંઝા', 'માર્કેટયાર્ડ') 4.345256717766886e-06\n",
      "('જુઓ:', 'ઊંઝા', 'માર્કેટયાર્ડ', 'આજથી') 4.345256717766886e-06\n",
      "('ઊંઝા', 'માર્કેટયાર્ડ', 'આજથી', '25') 4.345256717766886e-06\n",
      "('માર્કેટયાર્ડ', 'આજથી', '25', 'જુલાઈ') 4.345256717766886e-06\n",
      "('આજથી', '25', 'જુલાઈ', 'સુધી') 4.345256717766886e-06\n",
      "('25', 'જુલાઈ', 'સુધી', 'બંધ') 4.345256717766886e-06\n",
      "\n",
      "Reading file: unigram_toktype.pkl\n",
      "('આ',) 851661\n",
      "('વીડિયો',) 694606\n",
      "('જુઓ:',) 690600\n",
      "('ઊંઝા',) 690518\n",
      "('માર્કેટયાર્ડ',) 690433\n",
      "('આજથી',) 691383\n",
      "('25',) 692345\n",
      "('જુલાઈ',) 691089\n",
      "('સુધી',) 714280\n",
      "('બંધ',) 699245\n",
      "\n",
      "Reading file: bigram_toktype.pkl\n",
      "('<s>', 'આ') 758401\n",
      "('આ', 'વીડિયો') 691281\n",
      "('વીડિયો', 'જુઓ:') 690444\n",
      "('જુઓ:', 'ઊંઝા') 690409\n",
      "('ઊંઝા', 'માર્કેટયાર્ડ') 690409\n",
      "('માર્કેટયાર્ડ', 'આજથી') 690409\n",
      "('આજથી', '25') 690410\n",
      "('25', 'જુલાઈ') 690413\n",
      "('જુલાઈ', 'સુધી') 690521\n",
      "('સુધી', 'બંધ') 690544\n",
      "\n",
      "Reading file: trigram_toktype.pkl\n",
      "('<s>', '<s>', 'આ') 758401\n",
      "('<s>', 'આ', 'વીડિયો') 690895\n",
      "('આ', 'વીડિયો', 'જુઓ:') 690444\n",
      "('વીડિયો', 'જુઓ:', 'ઊંઝા') 690409\n",
      "('જુઓ:', 'ઊંઝા', 'માર્કેટયાર્ડ') 690409\n",
      "('ઊંઝા', 'માર્કેટયાર્ડ', 'આજથી') 690409\n",
      "('માર્કેટયાર્ડ', 'આજથી', '25') 690409\n",
      "('આજથી', '25', 'જુલાઈ') 690409\n",
      "('25', 'જુલાઈ', 'સુધી') 690409\n",
      "('જુલાઈ', 'સુધી', 'બંધ') 690413\n",
      "\n",
      "Reading file: quadrigram_toktype.pkl\n",
      "('<s>', '<s>', '<s>', 'આ') 758401\n",
      "('<s>', '<s>', 'આ', 'વીડિયો') 690895\n",
      "('<s>', 'આ', 'વીડિયો', 'જુઓ:') 690444\n",
      "('આ', 'વીડિયો', 'જુઓ:', 'ઊંઝા') 690409\n",
      "('વીડિયો', 'જુઓ:', 'ઊંઝા', 'માર્કેટયાર્ડ') 690409\n",
      "('જુઓ:', 'ઊંઝા', 'માર્કેટયાર્ડ', 'આજથી') 690409\n",
      "('ઊંઝા', 'માર્કેટયાર્ડ', 'આજથી', '25') 690409\n",
      "('માર્કેટયાર્ડ', 'આજથી', '25', 'જુલાઈ') 690409\n",
      "('આજથી', '25', 'જુલાઈ', 'સુધી') 690409\n",
      "('25', 'જુલાઈ', 'સુધી', 'બંધ') 690409\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def read_chunked_pickle(file_path, num_lines=10):\n",
    "    \"\"\"\n",
    "    Read a pickle file saved in batches and print the first `num_lines` n-grams.\n",
    "    \"\"\"\n",
    "    print(f\"\\nReading file: {file_path}\")\n",
    "    count_printed = 0\n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                batch = pickle.load(f)  # load one batch at a time\n",
    "                for ngram, value in batch.items():\n",
    "                    print(ngram, value)\n",
    "                    count_printed += 1\n",
    "                    if count_printed >= num_lines:\n",
    "                        return  # stop after printing required lines\n",
    "            except EOFError:\n",
    "                break  # end of file\n",
    "\n",
    "# ---------------- Example Usage ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    files_to_check = [\n",
    "        \"unigram_addone.pkl\",\n",
    "        \"bigram_addone.pkl\",\n",
    "        \"trigram_addone.pkl\",\n",
    "        \"quadrigram_addone.pkl\",\n",
    "        \"unigram_addk.pkl\",\n",
    "        \"bigram_addk.pkl\",\n",
    "        \"trigram_addk.pkl\",\n",
    "        \"quadrigram_addk.pkl\",\n",
    "        \"unigram_toktype.pkl\",\n",
    "        \"bigram_toktype.pkl\",\n",
    "        \"trigram_toktype.pkl\",\n",
    "        \"quadrigram_toktype.pkl\"\n",
    "    ]\n",
    "\n",
    "    for file in files_to_check:\n",
    "        read_chunked_pickle(file, num_lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b83f7b",
   "metadata": {},
   "source": [
    "<h1>applying the model on 1000 test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2564b966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentence probabilities saved to 'sentence_probabilities.csv'\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import math\n",
    "import csv\n",
    "\n",
    "# -------------------------------\n",
    "# Helper function to load models\n",
    "# -------------------------------\n",
    "def load_model(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# -------------------------------\n",
    "# Configuration\n",
    "# -------------------------------\n",
    "n_models = [\"unigram\", \"bigram\", \"trigram\", \"quadrigram\"]\n",
    "smoothings = [\"addone\", \"addk\", \"toktype\"]\n",
    "\n",
    "# Load all models dynamically into a dictionary\n",
    "models = {\n",
    "    f\"{n}_{smooth}\": load_model(f\"{n}_{smooth}.pkl\")\n",
    "    for n in n_models\n",
    "    for smooth in smoothings\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Probability calculator\n",
    "# -------------------------------\n",
    "def sentence_prob(sentence_tokens, model, n):\n",
    "    \"\"\"Compute raw & log probability of a sentence given an n-gram model.\"\"\"\n",
    "    prob = 1.0\n",
    "    log_prob = 0.0\n",
    "    padded = [\"<s>\"] * (n - 1) + sentence_tokens + [\"</s>\"]\n",
    "\n",
    "    for i in range(n - 1, len(padded)):\n",
    "        ngram = tuple(padded[i - n + 1 : i + 1])\n",
    "        p = model.get(ngram, 1e-12)  # fallback tiny value if missing\n",
    "        prob *= p\n",
    "        log_prob += math.log(p)      #adding the log probability to avoid underflow\n",
    "    return prob, log_prob\n",
    "\n",
    "# -------------------------------\n",
    "# Read test sentences\n",
    "# -------------------------------\n",
    "with open(\"test_sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sentences = [line.strip().split() for line in f]\n",
    "\n",
    "# -------------------------------\n",
    "# Calculate probabilities\n",
    "# -------------------------------\n",
    "results = []\n",
    "for sent in sentences:\n",
    "    res = {\"sentence\": \" \".join(sent)}\n",
    "    for n in n_models:\n",
    "        for smooth in smoothings:\n",
    "            model_key = f\"{n}_{smooth}\"\n",
    "            n_val = n_models.index(n) + 1  # unigram=1, bigram=2, etc.\n",
    "            res[model_key] = sentence_prob(sent, models[model_key], n_val)\n",
    "    results.append(res)\n",
    "\n",
    "# -------------------------------\n",
    "# Save results to CSV\n",
    "# -------------------------------\n",
    "with open(\"sentence_probabilities.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = [\"sentence\"]\n",
    "    for n in n_models:\n",
    "        for smooth in smoothings:\n",
    "            fieldnames += [f\"{n}_{smooth}_prob\", f\"{n}_{smooth}_logprob\"]\n",
    "\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for r in results:\n",
    "        row = {\"sentence\": r[\"sentence\"]}\n",
    "        for n in n_models:\n",
    "            for smooth in smoothings:\n",
    "                p, lp = r[f\"{n}_{smooth}\"]\n",
    "                row[f\"{n}_{smooth}_prob\"] = p\n",
    "                row[f\"{n}_{smooth}_logprob\"] = lp\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"✅ Sentence probabilities saved to 'sentence_probabilities.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f041e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>unigram_addone_prob</th>\n",
       "      <th>unigram_addone_logprob</th>\n",
       "      <th>unigram_addk_prob</th>\n",
       "      <th>unigram_addk_logprob</th>\n",
       "      <th>unigram_toktype_prob</th>\n",
       "      <th>unigram_toktype_logprob</th>\n",
       "      <th>bigram_addone_prob</th>\n",
       "      <th>bigram_addone_logprob</th>\n",
       "      <th>bigram_addk_prob</th>\n",
       "      <th>...</th>\n",
       "      <th>trigram_addk_prob</th>\n",
       "      <th>trigram_addk_logprob</th>\n",
       "      <th>trigram_toktype_prob</th>\n",
       "      <th>trigram_toktype_logprob</th>\n",
       "      <th>quadrigram_addone_prob</th>\n",
       "      <th>quadrigram_addone_logprob</th>\n",
       "      <th>quadrigram_addk_prob</th>\n",
       "      <th>quadrigram_addk_logprob</th>\n",
       "      <th>quadrigram_toktype_prob</th>\n",
       "      <th>quadrigram_toktype_logprob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>આજે અમદાવાદમાં હવામાન ખૂબ જ ગરમ છે.</td>\n",
       "      <td>3.131978e-34</td>\n",
       "      <td>-77.146228</td>\n",
       "      <td>3.734935e-34</td>\n",
       "      <td>-76.970163</td>\n",
       "      <td>1.306414e+29</td>\n",
       "      <td>67.042254</td>\n",
       "      <td>1.100848e-42</td>\n",
       "      <td>-96.612493</td>\n",
       "      <td>4.372406e-41</td>\n",
       "      <td>...</td>\n",
       "      <td>1.356943e-63</td>\n",
       "      <td>-144.757626</td>\n",
       "      <td>2.287596e-25</td>\n",
       "      <td>-56.737126</td>\n",
       "      <td>1.458486e-79</td>\n",
       "      <td>-181.526823</td>\n",
       "      <td>5.601282e-79</td>\n",
       "      <td>-180.181227</td>\n",
       "      <td>4.798937e-61</td>\n",
       "      <td>-138.889296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ભારતીય ક્રિકેટ ટીમે મેચ જીતી લીધી.</td>\n",
       "      <td>6.533053e-36</td>\n",
       "      <td>-81.016189</td>\n",
       "      <td>7.586069e-36</td>\n",
       "      <td>-80.866750</td>\n",
       "      <td>1.109357e+23</td>\n",
       "      <td>53.063238</td>\n",
       "      <td>2.456280e-34</td>\n",
       "      <td>-77.389245</td>\n",
       "      <td>1.474797e-32</td>\n",
       "      <td>...</td>\n",
       "      <td>7.163913e-51</td>\n",
       "      <td>-115.462783</td>\n",
       "      <td>2.278161e-13</td>\n",
       "      <td>-29.110238</td>\n",
       "      <td>5.401421e-60</td>\n",
       "      <td>-136.468444</td>\n",
       "      <td>4.024127e-59</td>\n",
       "      <td>-134.460213</td>\n",
       "      <td>3.299564e-31</td>\n",
       "      <td>-70.186347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>સરકારે નવી શિક્ષણ નીતિની જાહેરાત કરી.</td>\n",
       "      <td>1.161830e-35</td>\n",
       "      <td>-80.440482</td>\n",
       "      <td>1.339398e-35</td>\n",
       "      <td>-80.298258</td>\n",
       "      <td>1.121741e+23</td>\n",
       "      <td>53.074339</td>\n",
       "      <td>6.180491e-43</td>\n",
       "      <td>-97.189761</td>\n",
       "      <td>1.835523e-41</td>\n",
       "      <td>...</td>\n",
       "      <td>4.690165e-53</td>\n",
       "      <td>-120.491542</td>\n",
       "      <td>2.274777e-13</td>\n",
       "      <td>-29.111724</td>\n",
       "      <td>1.074730e-75</td>\n",
       "      <td>-172.621813</td>\n",
       "      <td>2.148011e-75</td>\n",
       "      <td>-171.929339</td>\n",
       "      <td>6.911490e-67</td>\n",
       "      <td>-152.340016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>શેર બજારમાં આજે તેજી જોવા મળી.</td>\n",
       "      <td>5.026476e-35</td>\n",
       "      <td>-78.975759</td>\n",
       "      <td>5.835135e-35</td>\n",
       "      <td>-78.826581</td>\n",
       "      <td>1.141304e+23</td>\n",
       "      <td>53.091628</td>\n",
       "      <td>3.662220e-29</td>\n",
       "      <td>-65.476898</td>\n",
       "      <td>3.922311e-27</td>\n",
       "      <td>...</td>\n",
       "      <td>5.373735e-47</td>\n",
       "      <td>-106.539976</td>\n",
       "      <td>1.569093e+05</td>\n",
       "      <td>11.963423</td>\n",
       "      <td>7.300742e-70</td>\n",
       "      <td>-159.192981</td>\n",
       "      <td>2.653842e-69</td>\n",
       "      <td>-157.902363</td>\n",
       "      <td>4.767060e-49</td>\n",
       "      <td>-111.264940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>આગામી અઠવાડિયે નવી ફિલ્મ રિલીઝ થશે.</td>\n",
       "      <td>1.774352e-34</td>\n",
       "      <td>-77.714458</td>\n",
       "      <td>2.061284e-34</td>\n",
       "      <td>-77.564564</td>\n",
       "      <td>1.129755e+23</td>\n",
       "      <td>53.081458</td>\n",
       "      <td>2.344175e-34</td>\n",
       "      <td>-77.435960</td>\n",
       "      <td>1.353665e-32</td>\n",
       "      <td>...</td>\n",
       "      <td>9.773927e-53</td>\n",
       "      <td>-119.757292</td>\n",
       "      <td>2.275468e-13</td>\n",
       "      <td>-29.111420</td>\n",
       "      <td>4.365154e-62</td>\n",
       "      <td>-141.286622</td>\n",
       "      <td>2.668692e-61</td>\n",
       "      <td>-139.476102</td>\n",
       "      <td>3.295230e-31</td>\n",
       "      <td>-70.187662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>વિકાસના કાર્યોને વેગ આપવા માટે કેન્દ્ર સરકારે ...</td>\n",
       "      <td>1.699204e-64</td>\n",
       "      <td>-146.835286</td>\n",
       "      <td>2.162622e-64</td>\n",
       "      <td>-146.594124</td>\n",
       "      <td>2.926201e+34</td>\n",
       "      <td>79.361598</td>\n",
       "      <td>1.186722e-89</td>\n",
       "      <td>-204.758879</td>\n",
       "      <td>7.943761e-88</td>\n",
       "      <td>...</td>\n",
       "      <td>5.255832e-121</td>\n",
       "      <td>-276.953458</td>\n",
       "      <td>3.291321e-91</td>\n",
       "      <td>-208.343954</td>\n",
       "      <td>1.000000e-144</td>\n",
       "      <td>-331.572253</td>\n",
       "      <td>1.000000e-144</td>\n",
       "      <td>-331.572253</td>\n",
       "      <td>1.000000e-144</td>\n",
       "      <td>-331.572253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>ભારતની મહિલા હોકી ટીમે એશિયન ગેમ્સમાં ગોલ્ડ મે...</td>\n",
       "      <td>4.313552e-67</td>\n",
       "      <td>-152.811440</td>\n",
       "      <td>5.180189e-67</td>\n",
       "      <td>-152.628360</td>\n",
       "      <td>1.194296e+58</td>\n",
       "      <td>133.727492</td>\n",
       "      <td>2.730453e-104</td>\n",
       "      <td>-238.464382</td>\n",
       "      <td>2.025489e-102</td>\n",
       "      <td>...</td>\n",
       "      <td>7.550708e-134</td>\n",
       "      <td>-306.524761</td>\n",
       "      <td>3.292742e-103</td>\n",
       "      <td>-235.974544</td>\n",
       "      <td>5.460555e-148</td>\n",
       "      <td>-339.085043</td>\n",
       "      <td>1.090663e-147</td>\n",
       "      <td>-338.393223</td>\n",
       "      <td>6.907840e-139</td>\n",
       "      <td>-318.126671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>આગામી બજેટમાં નાણા મંત્રી દ્વારા મોટા નિર્ણયો ...</td>\n",
       "      <td>3.842829e-49</td>\n",
       "      <td>-111.480461</td>\n",
       "      <td>5.058505e-49</td>\n",
       "      <td>-111.205599</td>\n",
       "      <td>3.065299e+52</td>\n",
       "      <td>120.854570</td>\n",
       "      <td>1.255164e-50</td>\n",
       "      <td>-114.901988</td>\n",
       "      <td>1.118341e-47</td>\n",
       "      <td>...</td>\n",
       "      <td>1.104421e-100</td>\n",
       "      <td>-230.159189</td>\n",
       "      <td>1.608961e-55</td>\n",
       "      <td>-126.166592</td>\n",
       "      <td>7.157109e-119</td>\n",
       "      <td>-272.039520</td>\n",
       "      <td>5.609494e-118</td>\n",
       "      <td>-269.980580</td>\n",
       "      <td>3.299798e-91</td>\n",
       "      <td>-208.341382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>શહેરીકરણની સમસ્યાઓને ઉકેલવા માટે નવી શહેરી વિક...</td>\n",
       "      <td>1.310153e-54</td>\n",
       "      <td>-124.069451</td>\n",
       "      <td>1.559412e-54</td>\n",
       "      <td>-123.895286</td>\n",
       "      <td>2.945801e+46</td>\n",
       "      <td>106.999295</td>\n",
       "      <td>8.936709e-91</td>\n",
       "      <td>-207.345076</td>\n",
       "      <td>2.300550e-89</td>\n",
       "      <td>...</td>\n",
       "      <td>1.593233e-125</td>\n",
       "      <td>-287.357371</td>\n",
       "      <td>6.904130e-115</td>\n",
       "      <td>-262.865166</td>\n",
       "      <td>1.000000e-132</td>\n",
       "      <td>-303.941232</td>\n",
       "      <td>1.000000e-132</td>\n",
       "      <td>-303.941232</td>\n",
       "      <td>1.000000e-132</td>\n",
       "      <td>-303.941232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>પર્યાવરણની સુરક્ષા માટે સરકાર અને સામાજિક સંસ્...</td>\n",
       "      <td>8.798855e-58</td>\n",
       "      <td>-131.375314</td>\n",
       "      <td>1.170266e-57</td>\n",
       "      <td>-131.090119</td>\n",
       "      <td>1.947611e+58</td>\n",
       "      <td>134.216539</td>\n",
       "      <td>3.688322e-83</td>\n",
       "      <td>-189.809391</td>\n",
       "      <td>1.129502e-80</td>\n",
       "      <td>...</td>\n",
       "      <td>1.709463e-134</td>\n",
       "      <td>-308.010223</td>\n",
       "      <td>3.291069e-103</td>\n",
       "      <td>-235.975052</td>\n",
       "      <td>1.738108e-149</td>\n",
       "      <td>-342.532382</td>\n",
       "      <td>3.331373e-149</td>\n",
       "      <td>-341.881794</td>\n",
       "      <td>6.904190e-139</td>\n",
       "      <td>-318.127199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  unigram_addone_prob  \\\n",
       "0                  આજે અમદાવાદમાં હવામાન ખૂબ જ ગરમ છે.         3.131978e-34   \n",
       "1                   ભારતીય ક્રિકેટ ટીમે મેચ જીતી લીધી.         6.533053e-36   \n",
       "2                સરકારે નવી શિક્ષણ નીતિની જાહેરાત કરી.         1.161830e-35   \n",
       "3                       શેર બજારમાં આજે તેજી જોવા મળી.         5.026476e-35   \n",
       "4                  આગામી અઠવાડિયે નવી ફિલ્મ રિલીઝ થશે.         1.774352e-34   \n",
       "..                                                 ...                  ...   \n",
       "995  વિકાસના કાર્યોને વેગ આપવા માટે કેન્દ્ર સરકારે ...         1.699204e-64   \n",
       "996  ભારતની મહિલા હોકી ટીમે એશિયન ગેમ્સમાં ગોલ્ડ મે...         4.313552e-67   \n",
       "997  આગામી બજેટમાં નાણા મંત્રી દ્વારા મોટા નિર્ણયો ...         3.842829e-49   \n",
       "998  શહેરીકરણની સમસ્યાઓને ઉકેલવા માટે નવી શહેરી વિક...         1.310153e-54   \n",
       "999  પર્યાવરણની સુરક્ષા માટે સરકાર અને સામાજિક સંસ્...         8.798855e-58   \n",
       "\n",
       "     unigram_addone_logprob  unigram_addk_prob  unigram_addk_logprob  \\\n",
       "0                -77.146228       3.734935e-34            -76.970163   \n",
       "1                -81.016189       7.586069e-36            -80.866750   \n",
       "2                -80.440482       1.339398e-35            -80.298258   \n",
       "3                -78.975759       5.835135e-35            -78.826581   \n",
       "4                -77.714458       2.061284e-34            -77.564564   \n",
       "..                      ...                ...                   ...   \n",
       "995             -146.835286       2.162622e-64           -146.594124   \n",
       "996             -152.811440       5.180189e-67           -152.628360   \n",
       "997             -111.480461       5.058505e-49           -111.205599   \n",
       "998             -124.069451       1.559412e-54           -123.895286   \n",
       "999             -131.375314       1.170266e-57           -131.090119   \n",
       "\n",
       "     unigram_toktype_prob  unigram_toktype_logprob  bigram_addone_prob  \\\n",
       "0            1.306414e+29                67.042254        1.100848e-42   \n",
       "1            1.109357e+23                53.063238        2.456280e-34   \n",
       "2            1.121741e+23                53.074339        6.180491e-43   \n",
       "3            1.141304e+23                53.091628        3.662220e-29   \n",
       "4            1.129755e+23                53.081458        2.344175e-34   \n",
       "..                    ...                      ...                 ...   \n",
       "995          2.926201e+34                79.361598        1.186722e-89   \n",
       "996          1.194296e+58               133.727492       2.730453e-104   \n",
       "997          3.065299e+52               120.854570        1.255164e-50   \n",
       "998          2.945801e+46               106.999295        8.936709e-91   \n",
       "999          1.947611e+58               134.216539        3.688322e-83   \n",
       "\n",
       "     bigram_addone_logprob  bigram_addk_prob  ...  trigram_addk_prob  \\\n",
       "0               -96.612493      4.372406e-41  ...       1.356943e-63   \n",
       "1               -77.389245      1.474797e-32  ...       7.163913e-51   \n",
       "2               -97.189761      1.835523e-41  ...       4.690165e-53   \n",
       "3               -65.476898      3.922311e-27  ...       5.373735e-47   \n",
       "4               -77.435960      1.353665e-32  ...       9.773927e-53   \n",
       "..                     ...               ...  ...                ...   \n",
       "995            -204.758879      7.943761e-88  ...      5.255832e-121   \n",
       "996            -238.464382     2.025489e-102  ...      7.550708e-134   \n",
       "997            -114.901988      1.118341e-47  ...      1.104421e-100   \n",
       "998            -207.345076      2.300550e-89  ...      1.593233e-125   \n",
       "999            -189.809391      1.129502e-80  ...      1.709463e-134   \n",
       "\n",
       "     trigram_addk_logprob  trigram_toktype_prob  trigram_toktype_logprob  \\\n",
       "0             -144.757626          2.287596e-25               -56.737126   \n",
       "1             -115.462783          2.278161e-13               -29.110238   \n",
       "2             -120.491542          2.274777e-13               -29.111724   \n",
       "3             -106.539976          1.569093e+05                11.963423   \n",
       "4             -119.757292          2.275468e-13               -29.111420   \n",
       "..                    ...                   ...                      ...   \n",
       "995           -276.953458          3.291321e-91              -208.343954   \n",
       "996           -306.524761         3.292742e-103              -235.974544   \n",
       "997           -230.159189          1.608961e-55              -126.166592   \n",
       "998           -287.357371         6.904130e-115              -262.865166   \n",
       "999           -308.010223         3.291069e-103              -235.975052   \n",
       "\n",
       "     quadrigram_addone_prob  quadrigram_addone_logprob  quadrigram_addk_prob  \\\n",
       "0              1.458486e-79                -181.526823          5.601282e-79   \n",
       "1              5.401421e-60                -136.468444          4.024127e-59   \n",
       "2              1.074730e-75                -172.621813          2.148011e-75   \n",
       "3              7.300742e-70                -159.192981          2.653842e-69   \n",
       "4              4.365154e-62                -141.286622          2.668692e-61   \n",
       "..                      ...                        ...                   ...   \n",
       "995           1.000000e-144                -331.572253         1.000000e-144   \n",
       "996           5.460555e-148                -339.085043         1.090663e-147   \n",
       "997           7.157109e-119                -272.039520         5.609494e-118   \n",
       "998           1.000000e-132                -303.941232         1.000000e-132   \n",
       "999           1.738108e-149                -342.532382         3.331373e-149   \n",
       "\n",
       "     quadrigram_addk_logprob  quadrigram_toktype_prob  \\\n",
       "0                -180.181227             4.798937e-61   \n",
       "1                -134.460213             3.299564e-31   \n",
       "2                -171.929339             6.911490e-67   \n",
       "3                -157.902363             4.767060e-49   \n",
       "4                -139.476102             3.295230e-31   \n",
       "..                       ...                      ...   \n",
       "995              -331.572253            1.000000e-144   \n",
       "996              -338.393223            6.907840e-139   \n",
       "997              -269.980580             3.299798e-91   \n",
       "998              -303.941232            1.000000e-132   \n",
       "999              -341.881794            6.904190e-139   \n",
       "\n",
       "     quadrigram_toktype_logprob  \n",
       "0                   -138.889296  \n",
       "1                    -70.186347  \n",
       "2                   -152.340016  \n",
       "3                   -111.264940  \n",
       "4                    -70.187662  \n",
       "..                          ...  \n",
       "995                 -331.572253  \n",
       "996                 -318.126671  \n",
       "997                 -208.341382  \n",
       "998                 -303.941232  \n",
       "999                 -318.127199  \n",
       "\n",
       "[1000 rows x 25 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('sentence_probabilities.csv')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
